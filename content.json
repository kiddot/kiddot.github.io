{"meta":{"title":"Share","subtitle":null,"description":null,"author":"kiddot","url":"https://kiddot.github.io"},"pages":[{"title":"Categories","date":"2017-12-12T09:25:50.380Z","updated":"2017-12-12T08:40:40.637Z","comments":true,"path":"categories/index.html","permalink":"https://kiddot.github.io/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2017-12-12T09:25:50.376Z","updated":"2017-12-12T08:40:40.637Z","comments":true,"path":"about/index.html","permalink":"https://kiddot.github.io/about/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-12-12T09:25:50.380Z","updated":"2017-12-12T08:40:40.637Z","comments":true,"path":"tags/index.html","permalink":"https://kiddot.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"学习Binder","slug":"Android/学习Binder","date":"2017-08-19T02:25:11.000Z","updated":"2018-03-05T16:45:35.315Z","comments":true,"path":"2017/08/19/Android/学习Binder/","link":"","permalink":"https://kiddot.github.io/2017/08/19/Android/学习Binder/","excerpt":"学习Binder Binder机制的目的是实现IPC(Inter-Process Communication)，即实现进程间通信。 知识储备","text":"学习Binder Binder机制的目的是实现IPC(Inter-Process Communication)，即实现进程间通信。 知识储备 1.Linux系统中内存的划分​ 在Linux系统中，应用程序都运行在用户空间，而Kernel和驱动都运行在内核空间。用户空间和内核空间若涉及到通信(即数据交互)，两者不能简单地使用指针传递数据，而必须在”内核”中copy_from_user(),copy_to_user(),get_user()或put_user()等函数传递数据。copy_from_user()和get_user()是将内核空间的数据拷贝到用户空间，而copy_to_user()和put_user()则是将用户空间的数据拷贝到内核空间。 2.进程的概念​ 进程可以说是程序运行在上面的抽象CPU，它拥有独立的内存单位，是系统进行资源分配和调度的基本单位。对Linux系统来说，每个用户空间运行的应用程序都可以看成一个进程。不同进程在不同内存当中。 3.代理模式-远程代理​ 远程代理是最经典的代理模式之一，远程代理负责与远程JVM通信，以实现本地调用者与远程被调用者之间的正常交互。 具体流程是这样的： Client向Stub发送方法调用请求（Client以为Stub就是Server） Stub接到请求，通过Socket与服务端的Skeleton通信，把调用请求传递给Skeleton Skeleton接到请求，调用本地Server（听起来有点奇怪，这里Server相当于Service） Server作出对应动作，把结果返回给调用者Skeleton Skeleton接到结果之后通过Socket发送给Stub Stub把结果传递给Client 4.内存映射 ​ 内存映射，简而言之就是将用户空间的一段内存区域映射到内核空间，映射成功后，用户对这段内存区域的修改可以直接反映到内核空间，相反，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间&lt;—-&gt;用户空间两者之间需要大量数据传输等操作的话效率是非常高的。 Binder的通信模型 上图中涉及到Binder模型的4类角色：Binder驱动，ServiceManager，Server和Client。 Binder驱动 两个不同进程的通信必须要内核进行中转，对于Android而言，在内核中起中转作用便是Binder驱动。 ​ Android的通信是基于Client-Server架构的，进程间的通信无非就是Client向Server发起请求，Server响应Client的请求。这里以发起请求为例：当Client向Server发起请求(例如，MediaPlayer向MediaPlayerService发起请求)，Client会先将请求数据从用户空间拷贝到内核空间(将数据从MediaPlayer发给Binder驱动)；数据被拷贝到内核空间之后，再通过驱动程序，将内核空间中的数据拷贝到Server位于用户空间的缓存中(Binder驱动将数据发给MediaPlayerService)。这样，就成功的将Client进程中的请求数据传递到了Server进程中。 ​ 实际上，Binder驱动是整个Binder机制的核心。除了实现上面所说的数据传输之外，Binder驱动还是实现线程控制(通过中断等待队列实现线程的等待/唤醒)，以及UID/PID等安全机制的保证。 ServiceManager ​ Binder是要实现Android的C-S架构的，即Client-Server架构。而ServiceManager，是以服务管理者的身份存在的。ServiceManager也是运行在用户空间的一个独立进程。 ​ 对于Binder驱动而言，ServiceManager是一个守护进程，更是Android系统各个服务的管理者。Android系统中的各个服务，都是添加到ServiceManager中进行管理的，而且每个服务都对应一个服务名。当Client获取某个服务时，则通过服务名来从ServiceManager中获取相应的服务。 ​ 对于MediaPlayerService和MediaPlayer而言，ServiceManager是一个Server服务端，是一个服务器。当要将MediaPlayerService等服务添加到ServiceManager中进行管理时，ServiceManager是服务器，它会收到MediaPlayerService进程的添加服务请求。当MediaPlayer等客户端要获取MediaPlayerService等服务时，它会向ServiceManager发起获取服务请求。 ​ 当MediaPlayer和MediaPlayerService通信时，MediaPlayerService是服务端；而当MediaPlayerService与ServiceManager通信时，ServiceManager则是服务端。这样，就造就了ServiceManager的特殊性。于是，在Binder驱动中，将句柄0指定为ServiceManager对应的句柄，通过这个特殊的句柄就能获取ServiceManager对象。 Client、Server ​ 这两个便是要进行通信的两个不同进程，比如MediaPlayerService和MediaPlayer 为什么选择Binder ​ Android是在Linux内核的基础上设计的。但是在Linux中，拥有”管道/消息队列/共享内存/信号量/Socket等等”众多的IPC通信手段。为什么不选择其它的IPC通信方式，而要设计出Binder？ 1.从传输效率上看，Binder更能实现C-S架构​ Linux支持的”传统的管道/消息队列/共享内存/信号量/Socket等”IPC通信手段中，只有Socket是Client-Server的通信方式。但是 ， socket作为一款通用接口，其传输效率低，开销大，主要用在跨网络的进程间通信和本机上进程间的低速通信。 2.从传输效率和可操作性上看 ​ 管道，消息队列采用存储-转发方式，即数据先从发送方缓存区拷贝到内核开辟的缓存区中，然后再从内核缓存区拷贝到接收方缓存区，至少有两次拷贝过程。效率太低！ ​ 对于共享内存来说，虽然使用它进行IPC通信时进行的内存拷贝次数是0。但是，共享内存操作复杂，并不适合用。 ​ 采用Binder机制的话，则只需要经过1次内存拷贝即可！ 即，从发送方的缓存区拷贝到内核的缓存区，而接收方的缓存区与内核的缓存区是映射到同一块物理地址的（内存映射），因此只需要1次拷贝即可。 从安全性来看，Binder更安全 ​ 传统IPC没有任何安全措施，完全依赖上层协议来确保。传统IPC的接收方无法获得对方进程可靠的UID/PID(用户ID/进程ID)，从而无法鉴别对方身份。而Binder机制则为每个进程分配了UID/PID来作为鉴别身份的标示，并且在Binder通信时会根据UID/PID进行有效性检测。 Binder中的各个联系 重要概念说明1.Binder实体 Binder实体，是各个Server以及ServiceManager在内核中的存在形式。 ​ Binder实体实际上是内核中binder_node结构体的对象，它的作用是在内核中保存Server和ServiceManager的信息(例如，Binder实体中保存了Server对象在用户空间的地址)。简言之，Binder实体是Server在Binder驱动中的存在形式，内核通过Binder实体可以找到用户空间的Server对象。 ​ 在上图中，Server和ServiceManager在Binder驱动中都对应的存在一个Binder实体。 2.Binder引用 所谓Binder引用，实际上是内核中binder_ref结构体的对象，它的作用是在表示”Binder实体”的引用。 ​ 每一个Binder引用都是某一个Binder实体的引用，通过Binder引用可以在内核中找到它对应的Binder实体。如果将Server看作是Binder实体的话，那么Client就好比Binder引用。Client要和Server通信，它就是通过保存一个Server对象的Binder引用，再通过该Binder引用在内核中找到对应的Binder实体，进而找到Server对象，然后将通信内容发送给Server对象。 ​ Binder实体和Binder引用都是内核(即，Binder驱动)中的数据结构。每一个Server在内核中就表现为一个Binder实体，而每一个Client则表现为一个Binder引用。这样，每个Binder引用都对应一个Binder实体，而每个Binder实体则可以多个Binder引用。 3.远程服务​ Server都是以服务的形式注册到ServiceManager中进行管理的。如果将Server本身看作是”本地服务”的话，那么Client中的”远程服务”就是本地服务的代理。远程服务就是本地服务的一个代理，通过该远程服务Client就能和Server进行通信。 简要流程分析ServiceManager守护进程​ ServiceManager是用户空间的一个守护进程。当该应用程序启动时，它会和Binder驱动进行通信，告诉Binder驱动它是服务管理者；对Binder驱动而言，它则会新建ServiceManager对应的Binder实体，并将该Binder实体设为全局变量。 ​ 为什么要将它设为全局变量呢？因为Client和Server都需和ServiceManager进行通信，不将它设为全局变量的话，无法找到ServiceManager Server注册到ServiceManager​ Server首先会向Binder驱动发起注册请求，而Binder驱动在收到该请求之后就将该请求转发给ServiceManager进程。但是Binder驱动怎么才能知道该请求是要转发给ServiceManager的呢？这是因为Server在发送请求的时候，会告诉Binder驱动这个请求是交给0号Binder引用对应的进程来进行处理的。而Binder驱动中指定了0号引用是与ServiceManager对应的。 ​ 在Binder驱动转发该请求之前，它其实还做了两件很重要的事： (01) 当它知道该请求是由一个Server发送的时候，它会新建该Server对应的Binder实体。 (02) 它在ServiceManager的”保存Binder引用的红黑树”中查找是否存在该Server的Binder引用；找不到的话，就新建该Server对应的Binder引用，并将其添加到”ServiceManager的保存Binder引用的红黑树”中。 ​ 当ServiceManager收到Binder驱动转发的注册请求之后，它就将该Server的相关信息注册到”Binder引用组成的单链表”中。这里所说的Server相关信息主要包括两部分：Server对应的服务名 + Server对应的Binder实体的一个Binder引用。 Client获取远程服务 Client要和某个Server通信，需要先获取到该Server的远程服务。 ​ Client首先会向Binder驱动发起获取服务的请求。Binder驱动在收到该请求之后也是该请求转发给ServiceManager进程。ServiceManager在收到Binder驱动转发的请求之后，会从”Binder引用组成的单链表”中找到要获取的Server的相关信息。 ​ 至于ServiceManager是如何从单链表中找到需要的Server的呢？答案是Client发送的请求数据中，会包括它要获取的Server的服务名；而ServiceManager正是根据这个服务名来找到Server的。 ​ 接下来，ServiceManager通过Binder驱动将Server信息反馈给Client的。它反馈的信息是Server对应的Binder实体的Binder引用信息。而Client在收到该Server的Binder引用信息之后，就根据该Binder引用信息创建一个Server对应的远程服务。这个远程服务就是Server的代理，Client通过调用该远程服务的接口，就相当于在调用Server的服务接口一样；因为Client调用该Server的远程服务接口时，该远程服务会对应的通过Binder驱动和真正的Server进行交互，从而执行相应的动作。 Binder的设计 通过上面，已经有了Binder模型的理论基础。现在开始学习它的设计思路。 两个中心思想一、Server提供接入点 ​ 如果C-S架构中的Client和Server属于同一进程的话，那么Client和Server之间的通信将非常容易。只需要在Client端先获取相应的Server端对象；然后，再通过Server对象调用Server的相应接口即可。但是，Binder机制中涉及到的Client和Server是位于不同的进程中的，这也就意味着，不可能直接获取到Server对象。那就需要Server提供一个接入点给Client。 而这个接入点就是“Server的远程服务代理”！ ​ Client能够获取到Server的远程服务，它就相当于Server的代理。Client要和Server通信时，它只需要调用该远程服务的相应接口即可，其他的工作都交给远程服务来处理。远程服务收到Client请求之后，会和Binder驱动通信；因为远程服务中有Server在Binder驱动中的Binder引用信息，因此远程服务就能轻易的找到对应的Server，进而将Client的请求内容发送Server。 二、通信协议 Binder机制中，涉及到大量的”内核的Binder驱动 和 用户空间的引用程序”之间的通信。需要指定对应的通信协议，确保通信的安全和正常。 内核空间的设计 ​ 内核空间的Binder设计涉及到3个非常重要的结构体：binder_proc，binder_node和binder_ref。 binder_proc 描述进程上下文信息的，每一个用户空间的进程都对应一个binder_proc结构体。 binder_node Binder实体对应的结构体，它是Server在Binder驱动中的体现 binder_ref Binder引用对应的结构体，它是Client在Binder驱动中的体现 如上图所示，binder_proc中包含了3棵红黑树。​ (01) Binder实体红黑树是保存”binder_proc对应的进程”所包含的Binder实体的，而Binder实体是与Server的服务对应的。可以将Binder实体红黑树理解为Server进程中包行的Server服务的红黑树。​ (02) 图中有两棵Binder引用红黑树，这两棵树所包含的Binder引用都是一样的。不同的是，红黑树的排序基准不同，一个是以Binder实体来排序，而另一个则是以Binder引用描述(Binder引用描述实际上就是一个32位的整型数)来排序。以Binder引用描述的红黑树是为了方便进行快速查找。 ​ 上图是描述Binder驱动中Binder实体结构体的。如图所示，Binder实体中有一个Binder引用的哈希表，专门来存放该Binder实体的Binder引用。这也如我们之前所说，每个Binder实体则可以多个Binder引用，而每个Binder引用则都只对应一个Binder实体。 用户空间的Binder设计 ​ Server是以服务的形式注册到ServiceManager中，而Server在Client中则是以远程服务的形式存在的。因此，这个图的主干就是理清楚本地服务和远程服务这两者之间的关系。 ​ “本地服务”就是Server提供的服务本身，而”远程服务”就是服务的代理；”服务接口”则是抽象出了它们的通用接口。这3个角色都是通用的，对于不同的服务而言，它们的名称都不相同。例如，对于MediaPlayerService服务而言，本地服务就是MediaPlayerService自身，远程服务是BpMediaPlayerService，而服务接口是IMediaPlayerService。当Client需要向MediaPlayerService发送请求时，它需要先获取到服务的代理(即，远程服务对象)，也就是BpMediaPlayerService实例，然后通过该实例和MediaPlayerService进行通信。 图中的ProcessState和IPCThreadState都是采用单例模式实现的，它们的实例都是全局的，而且只有唯一一个。 ​ (01) 当Server启动之后，它会先将自己注册到ServiceManager中。注册时，Binder驱动会创建Server对应的Binder实体，并将”Server对应的本地服务对象的地址”保存到Binder实体中。注册成功之后，Server就进入消息循环，等待Client的请求。​ (02) 当Client需要和Server通信时，会先获取到Server接入点，即获取到远程服务对象；而且Client要获取的远程服务对象是”服务接口”类型的。Client向ServiceManager发送获取服务的请求时，会通过IPCThreadState和Binder驱动进行通信；当ServiceManager反馈之后，IPCThreadState会将ServiceManager反馈的”Server的Binder引用信息”保存BpBinder中(具体来说，BpBinder的mHandle成员保存的就是Server的Binder引用信息)。然后，会根据该BpBinder对象创建对应的远程服务。这样，Client就获取到了远程服务对象，而且远程服务对象的成员中保存了Server的Binder引用信息。​ (03) 当Client获取到远程服务对象之后，它就可以轻松的和Server进行通信了。当它需要向Server发送请求时，它会调用远程服务接口；远程服务能够获取到BpBinder对象，而BpBinder则通过IPCThreadState和Binder驱动进行通信。由于BpBinder中保存了Server在Binder驱动中的Binder引用；因此，IPCThreadState和Binder驱动通信时，是知道该请求是需要传给哪个Server的。Binder驱动通过Binder引用找到对应的Binder实体，然后将Binder实体中保存的”Server对应的本地服务对象的地址”返回给用户空间。当IPC收到Binder驱动反馈的内容之后，它从内容中找到”Server对应的本地服务对象”，然后调用该对象的onTransact()。不同的本地服务都可以实现自己的onTransact()；这样，不同的服务就可以按照自己的需求来处理请求。 ###","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"项目组件化流程","slug":"Android/项目组件化","date":"2017-08-16T02:22:11.000Z","updated":"2018-03-05T16:45:08.366Z","comments":true,"path":"2017/08/16/Android/项目组件化/","link":"","permalink":"https://kiddot.github.io/2017/08/16/Android/项目组件化/","excerpt":"项目组件化 什么是组件化开发？就是将一个app分成多个模块，每个模块都是一个组件（Module），开发的过程中我们可以让这些组件相互依赖或者单独调试部分组件等，但是最终发布的时候是将这些组件合并统一成一个apk，这就是组件化开发。","text":"项目组件化 什么是组件化开发？就是将一个app分成多个模块，每个模块都是一个组件（Module），开发的过程中我们可以让这些组件相互依赖或者单独调试部分组件等，但是最终发布的时候是将这些组件合并统一成一个apk，这就是组件化开发。 为什么要组件化开发？比较常见的架构都是一个界面就存在大量的业务逻辑，而业务逻辑中又有各种网络请求以及数据操作等，整个项目没有模块的概念，基本都是以业务逻辑划分文件夹来进行开发，这样的做法最大的缺点就是业务之间会比较容易产生耦合。 随着产品版本的迭代，业务越来越复杂之后，带来的问题会更明显： 单一工程业务逻辑耦合度太高，容易改动一个地方会牵扯到其它本不应该相关的业务逻辑。 随着工程越来越大，编译的时间会越来越久。 团队协作开发会存在较多的冲突。 不能灵活的对工程进行配置和组装.比如今天产品经理说加上这个功能,明天又说去掉,后天在加上. 什么是组件化开发在单工程模型的基础上，将业务层中的各业务抽取出来，封装成响应的业务组件，将基础库的各部分抽取，封装成基础组件，而主工程就是一个可运行的app，作为各组件的入口。业务组件在组件模式下可以独立开发，而在集成模式下又可以变为arr包集成到“app壳工程”中，组成一个完整功能的APP 上图是组件化工程模型，下面是组件化工程中用到的名词的含义： 名词 含义 集成模式 所有的业务组件被“app壳工程”依赖，组成一个完整的APP； 组件模式 可以独立开发业务组件，每一个业务组件就是一个APP； app壳工程 负责管理各个业务组件，和打包apk，没有具体的业务功能； 业务组件 根据公司具体业务而独立形成一个的工程； 功能组件 提供开发APP的某些基础功能，例如打印日志、树状图等； Main组件 属于业务组件，指定APP启动页面、主界面； Common组件 属于功能组件，支撑业务组件的基础，提供多数业务组件需要的功能，例如提供网络请求功能； 组件化工程模型中，看似业务组件之间相对是独立的，但实际开发中多少会有些情况需要组件之间相互通信，那么这如果处理不好可能会带来一些依赖关系变得十分复杂的情况，甚至会出现一些潜在的风险，但是这种模型带来的好处还是十分明显的： 1.加快业务的迭代速度，各业务模块更加独立。 2.加快编译速度，提高开发效率 3.降低团队成员熟悉项目成本，降低项目维护难度 组件化具体流程1.第一步：配置可自动将组件在Application和Library属性之间切换在Android studio中的Module主要有两种属性，分别为： Application属性，可以独立运行android程序 apply plugin: ‘com.android.application’ library属性：不可以独立运行，一般是android程序依赖的库文件 apply plugin: ‘com.android.library’ 当我们在开发单独组件的时候，这个组件应该处于application模式，而当我们要将单独组件合并到主工程的时候，就需要将单独组从application模式改为library模式。为了做到一次修改，全局组件生效。在Android Studio项目的根目录下有一个gradle.properties 文件，这个文件主要用来配置Gradle settings。在这里gradle.properties添加了一行代码，定义一个属性isModule（是否是组件开发模式，true为是，false为否）： 1234if (isModule.toBoolean()) &#123; apply plugin: 'com.android.application'&#125; else &#123; apply plugin: 'com.android.library'&#125; 2.第二步：解决组件AndroidManifest和主工程AndroidManifest合并问题 每个组件是由不同的成员单独开发的，这个时候组件就是一个独立的APP，那么这个组件就会有自己的“AndroidManifest.xml”，但是Android程序只有一个“AndroidManifest.xml”，当我们要把组件作为Library合并到主工程的时候，组件的“AndroidManifest.xml”和主工程的“AndroidManifest.xml”就会产生冲突，因为他们都有自己实现application类以及一些属性，还有自己的MAIN Activity，如果直接把张表合并到一起势必产生冲突。 解决思路就是：每个组件维护两张表，一张用于组件单独开发时使用，另一张用于合并到主工程的注册表中，每当增加一个Android系统的四大组件时都要同时给两张表中添加。 首先在组件的main文件夹（和java文件夹平级）下创建两个文件夹: 然后在每个组件的build.gradle中添加如下的代码： 12345678910111213sourceSets &#123; main &#123; if (isModule.toBoolean()) &#123; manifest.srcFile 'src/main/debug/AndroidManifest.xml' &#125; else &#123; manifest.srcFile 'src/main/release/AndroidManifest.xml' //release模式下排除debug文件夹中的所有Java文件 java &#123; exclude 'debug/**' &#125; &#125; &#125;&#125; 这些代码的意思是：当在组件开发模式下，组件的注册表文件使用debug文件夹下的，其他情况使用release文件夹下的注册表文件；那么这两张表的区别在哪里？ 下面表示debug文件夹中的： 123456789101112131415161718192021&lt;application android:name=\"debug.CarApplication\" android:icon=\"@mipmap/ic_car_launcher\" android:label=\"@string/car_name\" android:supportsRtl=\"true\" android:theme=\"@style/AppTheme\"&gt; &lt;activity android:name=\".query.QueryActivity\" android:configChanges=\"orientation|screenSize|keyboard\" android:screenOrientation=\"portrait\" android:windowSoftInputMode=\"adjustPan|stateHidden\"&gt; &lt;intent-filter&gt; &lt;action android:name=\"android.intent.action.MAIN\" /&gt; &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt; intent-filter&gt; activity&gt; &lt;activity android:name=\".scan.ScanActivity\" android:screenOrientation=\"portrait\" /&gt;application&gt; 下面是release文件夹中的： 123456789101112 &lt;application android:theme=\"@style/AppTheme\"&gt; &lt;activity android:name=\".query.QueryActivity\" android:configChanges=\"orientation|screenSize|keyboard\" android:screenOrientation=\"portrait\" android:theme=\"@style/AppTheme\" android:windowSoftInputMode=\"adjustPan|stateHidden\" /&gt; &lt;activity android:name=\".scan.ScanActivity\" android:screenOrientation=\"portrait\" /&gt;application&gt; debug文件夹中注册表的标签中指定了具体application类，而release文件夹中的则没有， debug文件夹中注册表的标签中添加一些application属性，而release文件夹中的则什么都没有添加； debug文件夹中的注册表指定QueryActivity为MAIN Activity，也就是要启动的 Activity，而release文件夹中的则没有； 3.第三步：解决组件和主工程的Application冲突问题以及组件单独开发初始化数据问题 当android程序启动时，android系统会为每个程序创建一个Application类的对象，并且只创建一个，application对象的生命周期是整个程序中最长的，它的生命周期就等于这个程序的生命周期。在默认情况下应用系统会自动生成Application 对象，但是如果我们自定义了Application，那就需要告知系统，实例化的时候，是实例化我们自定义的，而非默认的。 Q:我们在组件化开发的时候每一个组件可能都会有一个自己的Application类的对象，如果我们在自己的组件中开发时需要获取全局的Context，一般都会直接获取application对象，但是当所有组件要打包合并在一起的时候就会出现问题，因为最后程序只有一个Application，我们组件中自己定义的Application肯定没法使用 A:首先创建一个叫做Common的Library，这个Common库中主要包含整个项目用到公共基类、工具类、自定义View等，例如BaseActivity、BaseFragment、BaseApplication等，并且我们的每一个组件都要依赖这个Common库，现在主要讲Common库中的BaseApplication怎么定义，下面是BaseApplication中的部分代码： 12345678910111213141516171819public class BaseApplication extends Application &#123; private static BaseApplication sInstance; public static Context context; public static BaseApplication getIns() &#123; return sInstance; &#125; @Override public void onCreate() &#123; super.onCreate(); sInstance = this; context = this.getApplicationContext(); if (isAppDebug(context)) &#123; //只有debug模式才会打印日志 Logger.init(\"Demo\").logLevel(LogLevel.FULL); &#125; else &#123; Logger.init(\"Demo\").logLevel(LogLevel.NONE); &#125; &#125; &#125; 因为每个组件都依赖了Common库，所以每个组件都能够获取到BaseApplication.context，但是Android程序默认的是系统自己的Application这个类，要想使用自己的就要继承Application并且在AndroidManifest.xml中声明，因此我们先在自己的组件中创建一个组件Application并且继承于BaseApplication，然后在debug文件中的AndroidManifest.xml中声明： 12345678public class CarApplication extends BaseApplication &#123; @Override public void onCreate() &#123; super.onCreate(); login(); &#125;&#125; 这样我们就可以在组件中使用全局的Context：BaseApplication.context了，但是还有一个问题，我们在自己的组件中定义了CarApplication，那么组件合并到主工程后，主工程也有自己的Application，这样又冲突了。一个解决思路跟上面类似，合并的时候排除掉就可以了，主要做法如下： 我们在java文件夹下再建一个debug文件夹，把组件自己的application放在这个文件夹中，然后在build.gradle添加这行代码： 12345678910111213sourceSets &#123; main &#123; if (isModule.toBoolean()) &#123; manifest.srcFile 'src/main/debug/AndroidManifest.xml' &#125; else &#123; manifest.srcFile 'src/main/release/AndroidManifest.xml' //release模式下排除debug文件夹中的所有Java文件 java &#123; exclude 'debug/**'//增加这里的代码 &#125; &#125; &#125;&#125; 这样在合并到主项目时debug文件夹下的java文件就全部被排除了。并且你可以在组件的Application中做一些初始化的操作，比如登陆，然后把数据保存下来，供组件使用。 4.第四步：解决library重复依赖以及SDK和依赖的第三方库版本号控制问题 重复依赖问题其实在开发中经常会遇到，比如你 compile 了一个A，然后在这个库里面又 compile 了一个B，然后你的工程中又 compile 了一个同样的B，就依赖了两次。 默认情况下，如果是 aar 依赖，gradle 会自动帮我们找出新版本的库而抛弃旧版本的重复依赖。但是如果你使用的是 project 依赖，gradle 并不会去去重，最后打包就会出现代码中有重复的类了。 Library重复依赖的解决办法就是给整个工程提供统一的依赖第三方库的入口,可以建立一个Common库，这个库还有一个作用就是用来为整个项目提供统一的依赖第三方库的入口，我们把项目常用或者必须用到的库全部在Common库的build.gradle中依赖进来，例如Android support Library、网络库、图片加载库等，又因为每个组件都要依赖这个Common库，所以的build.gradle中就不在需要依赖任何其他库了，这样我们就有了统一的依赖第三方库的入口，添加、删除和升级库文件都只需要在Common库中去处理就好了。 另外一个问题就是我们每个组件的build.gradle中都要配置一些属性，例如compileSdkVersion、buildToolsVersion还有defaultConfig等，如果我们需要修改项目的compileSdkVersion版本号，那就麻烦了，因为将会有很多组的build.gradle。所以要把这些build.gradle中都要配置的属性统一起来，类似于java中的静态常量，一处修改到处生效。在项目的build.gradle中： 1234567891011121314151617181920212223242526272829// Define versions in a single placeext &#123;// Sdk and tools buildToolsVersion = localBuildToolsVersion compileSdkVersion = 23 minSdkVersion = 16 targetSdkVersion = 23//时间：2017.2.13；每次修改版本号都要添加修改时间 versionCode = 1 versionName = \"1.0\" javaVersion = JavaVersion.VERSION_1_8 / App dependencies version supportLibraryVersion = \"23.2.1\" retrofitVersion = \"2.1.0\" glideVersion = \"3.7.0\" loggerVersion = \"1.15\" eventbusVersion = \"3.0.0\" gsonVersion = \"2.8.0\"&#125; 然后在组件的build.gradle中引用这些值，下面是Common库的build.gradle代码会和组件的build.gradle有些许差异： 12345678910111213141516171819202122232425262728293031323334353637apply plugin: 'com.android.library'android &#123; compileSdkVersion rootProject.ext.compileSdkVersion buildToolsVersion rootProject.ext.buildToolsVersiondefaultConfig &#123; minSdkVersion rootProject.ext.minSdkVersion targetSdkVersion rootProject.ext.targetSdkVersion versionCode rootProject.ext.versionCode versionName rootProject.ext.versionName&#125;buildTypes &#123; release &#123; minifyEnabled false proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro' &#125; &#125;&#125;dependencies &#123; compile fileTree(dir: 'libs', include: ['*.jar']) //Android Support compile \"com.android.support:appcompat-v7:$rootProject.supportLibraryVersion\" compile \"com.android.support:design:$rootProject.supportLibraryVersion\" compile \"com.android.support:percent:$rootProject.supportLibraryVersion\" //网络请求相关 compile \"com.squareup.retrofit2:retrofit:$rootProject.retrofitVersion\" compile \"com.squareup.retrofit2:retrofit-mock:$rootProject.retrofitVersion\" compile \"com.github.franmontiel:PersistentCookieJar:$rootProject.cookieVersion\" //稳定的 compile \"com.github.bumptech.glide:glide:$rootProject.glideVersion\" compile \"com.orhanobut:logger:$rootProject.loggerVersion\" compile \"org.greenrobot:eventbus:$rootProject.eventbusVersion\" compile \"com.google.code.gson:gson:$rootProject.gsonVersion\" //不稳定的 compile \"com.github.mzule.activityrouter:activityrouter:$rootProject.routerVersion\" compile \"com.jude:easyrecyclerview:$rootProject.easyRecyclerVersion\"&#125; 这样我们修改compileSdkVersion、buildToolsVersion、defaultConfig的值或者依赖库文件的版本号都可以直接在项目的build.gradle文件中直接修改了，修改完后整个项目也就都改过来了。 5.第五步：跨Module跳转问题 在组件化开发的时候，我们不能在使用显示调用来跳转页面了，因为我们组件化的目的之一就是解决模块间的强依赖问题，组件跟组件之间完全没有任何依赖，假如现在我从A组件跳转到B组件，并且要携带参数跳转，这时候怎么办？ 这个时候可以引入“路由”的概念。关于路由，有很多开源的库，比如Arouter，ActivityRouter等，可以根据自己需要选择一个。 6.第六步：Module之间通信问题如果在B组件中要通知A组件刷新列表，就要想办法解决组件间的通信问题，这个使用EventBus就能解决。 7.资源名冲突问题因为我们拆分出了很多组件，在合并到主工程的时候就有可能会出现资源名冲突问题，比如A组件和B组件都定义了同一个资源名。这个可以在组件的build.gradle中添加如下代码： 1resourcePrefix \"组件名_\" 设置了这个属性后有个问题，所有的资源名必须以指定的字符串做前缀，否则会报错，而且resourcePrefix这个值只能限定xml里面的资源，并不能限定图片资源，所有图片资源仍然需要手动去修改资源名。其实最好还是团队开发中增加资源命名规约，只要遵守这个命名规约就能规避资源名冲突问题。 组件化中可能会遇到的坑关于butterknife butterknife在library activity中的使用和注意事项 1、用R2代替R findviewid 2、在click方法中同样使用R2，但是找id的时候使用R。 3、特别注意library中switch-case的使用，在library中是不能使用switch- case 找id的，解决方法就是用if-else代替。 为什么要用R2代替R，而不能直接用R？因为在library里R.id.xxx不再是final类型了，也就不是常量了变成可变的，而注入是需要传入常量。R2则是从主工程中复制了一份R过来给library使用。 关于结言模块化是一个繁琐，枯燥，耗费时间长，通俗来说，时间和成果不成正比。进行模块化之前一定要思考是否项目真的需要，是不是单纯为了炫技。","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"进程回收策略","slug":"Android/进程回收策略","date":"2017-08-13T08:22:11.000Z","updated":"2018-03-05T16:44:33.409Z","comments":true,"path":"2017/08/13/Android/进程回收策略/","link":"","permalink":"https://kiddot.github.io/2017/08/13/Android/进程回收策略/","excerpt":"进程回收策略","text":"进程回收策略 笔记参考自：https://yq.aliyun.com/articles/201636?utm_content=m_30411 LowMemoryKiller(低内存杀手)是Andorid基于oomKiller原理所扩展的一个多层次oomKiller，OOMkiller(Out Of Memory Killer)是在Linux系统无法分配新内存的时候，选择性杀掉进程，到oom的时候，系统可能已经不太稳定，而LowMemoryKiller是一种根据内存阈值级别触发的内存回收的机制，在系统可用内存较低时，就会选择性杀死进程的策略，相对OOMKiller，更加灵活。 OOMKiller ​ 当我们启动应用时，需要向系统申请内存，即进行malloc的操作，进行malloc操作如果返回一个非NULL的操作表示申请到了可用的内存。事实上，这个地方是可能存在bug的。Linux有一种内存优化机制，即：允许程序申请比系统可用内存更多的内存（术语：overcommit），但是Linux并不保证这些内存马上可用，如果凑巧你申请到的内存中在你需要使用的时候还没有完全释放出来，这个时候就会触发OOM killer了。 ​ 系统的物理内存往往是有限的，这就需要在使用过程中杀掉一些无用的进程以腾出新的内存。在Android系统中，AmS需要和Linux操作系统有个约定，即将要谈到的Linux内核的内存管理控制系统是如何通知AMS内存不足的。 ​ Java虚拟机运行时都有各自独立的内存空间，应用程序A发生Out Of Memory并不意味着应用程序B也会发生Out Of Memory，很有可能仅仅是A程序用光了自己内存的上限，而系统内存却还是有的。所以说，单纯的AmS是无法获知系统内存是否低的。 ​ Android系统如何知道系统内存低或者系统内存不够了？因为Android底层的Linux未采用磁盘虚拟内存机制，所以应用程序能够使用的内存大小完全取决于实际物理内存大小。 ​ ​ 在Android中运行了一个OOM 进程，即Out Of Memory。该进程启动时会首先向Linux内核中把自己注册为一个OOM Killer，即当Linux内核的内存管理模块检测到系统内存低的时候就会通知已经注册的OOM进程，然后这些OOM Killer就可以根据各种规则进行内存释放了，当然也可以什么都不做。 ​ Android中的OOM Killer进程是仅仅适用于Android应用程序的，该进程在运行时，AmS需要把每一个应用程序的oom_adj值告知给Killer。这个值的范围在－16到15，值越低，说明越重要，这个值类似于Linux系统中的进程nice值，只是在标准的Linux中，有其自己的一套Killer机制。 LowMemoryKiller在理解OOMKiller的时候注意两点： LowMemoryKiller是被动杀死进程； Android应用通过AMS，利用proc文件系统更新进程信息。 Android 优先级更新 ​ APP中很多操作都可能会影响进程列表的优先级，比如退到后台、移到前台等，都会潜在的影响进程的优先级，我们知道Lowmemorykiller是通过遍历内核的进程结构体队列，选择优先级低的杀死，那么APP操作是如何写入到内核空间的呢？Linxu有用户间跟内核空间的区分，无论是APP还是系统服务，都是运行在用户空间，严格说用户控件的操作是无法直接影响内核空间的，更不用说更改进程的优先级。 ​ 其实这里是通过了Linux中的一个proc文件体统，proc文件系统可以简单的看成是内核空间映射成用户可以操作的文件系统，当然不是所有进程都有权利操作，通过proc文件系统，用户空间的进程就能够修改内核空间的数据，比如修改进程的优先级。 ​ 在Android5.0之前的系统是AMS进程直接修改的，5.0之后，是修改优先级的操作被封装成了一个独立的服务-lmkd，lmkd服务位于用户空间，其作用层次同AMS、WMS类似，就是一个普通的系统服务。 模拟一个场景，APP只有一个Activity，我们主动finish掉这个Activity，APP就回到了后台，这里要记住，虽然没有可用的Activity，但是APP本身是没哟死掉的，这就是所谓的热启动 针对4.3系统的流程如下： 针对5.0以上的系统，流程如下： LowMemoryKiller内核区块 LomemoryKiller属于一个内核驱动模块，主要功能是：在系统内存不足的时候扫描进程队列，找到低优先级（也许说性价比低更合适）的进程并杀死，以达到释放内存的目的。 LomemoryKiller是如何找到低优先级进程，并杀死的。管家代码就在lowmem_shrink函数里面： 1234567891011121314151617181920212223242526272829303132333435363738394041static int lowmem_shrink(int nr_to_scan, gfp_t gfp_mask)&#123; struct task_struct *p; 。。。 关键点1 找到当前的内存对应的阈值 for(i = 0; i &lt; array_size; i++) &#123; if (other_free &lt; lowmem_minfree[i] &amp;&amp; other_file &lt; lowmem_minfree[i]) &#123; min_adj = lowmem_adj[i]; break; &#125; &#125; 。。。 关键点2 找到优先级低于这个阈值的进程，并杀死 read_lock(&amp;tasklist_lock); for_each_process(p) &#123; if (p-&gt;oomkilladj &lt; min_adj || !p-&gt;mm) continue; tasksize = get_mm_rss(p-&gt;mm); if (tasksize &lt;= 0) continue; if (selected) &#123; if (p-&gt;oomkilladj &lt; selected-&gt;oomkilladj) continue; if (p-&gt;oomkilladj == selected-&gt;oomkilladj &amp;&amp; tasksize &lt;= selected_tasksize) continue; &#125; selected = p; selected_tasksize = tasksize; &#125; if(selected != NULL) &#123; force_sig(SIGKILL, selected); rem -= selected_tasksize; &#125; lowmem_print(4, \"lowmem_shrink %d, %x, return %d\\n\", nr_to_scan, gfp_mask, rem); read_unlock(&amp;tasklist_lock); return rem;&#125; 通过给应用设置内存对应的阈值，通过Linux的中的信号量，发送SIGKILL信号直接将进程杀死。","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"OKHttp总结","slug":"Android/OKHttp","date":"2017-08-11T08:22:11.000Z","updated":"2018-03-05T16:43:55.509Z","comments":true,"path":"2017/08/11/Android/OKHttp/","link":"","permalink":"https://kiddot.github.io/2017/08/11/Android/OKHttp/","excerpt":"OKHttpOKHttp是什么？ 是一个网络请求库。处理对网络的请求操作。","text":"OKHttpOKHttp是什么？ 是一个网络请求库。处理对网络的请求操作。 它做了一些什么操作？（我将从一步步做出一个简易OKHttp的角度来阐述） 首先，先思考没有使用库的情况下，一般是如何进行网络请求的。 接收用户的请求 -&gt; 发出请求 -&gt; 接收响应结果并返回给用户。 这种是最简单的请求情况，但是实际上应用的网络请求比较复杂。因此库的存在就是能有效帮助用户解决网络请求中可能出现的问题以及扩展请求的功能提高请求速度等等。 而OKHttp这个库，就是通过分层设计方法，把请求的流程分为七层。而层次之间的联系通过拦截器和组成链来实现。而且提供了自定义拦截器，用户可以在请求流程中加入自己想要处理的逻辑。例如加入日志打印等。 在这七层中的处理的，都是根据上层传入的request请求来进行自己的处理包装，然后传递给下一层。而每一层的处理有所不同，但又层层依赖。这种代码的层次风格很适合维护和降低开发难度。 拦截器层次结构假设没有拦截器去一层层包装request请求，可能就直接根据用户传进来的request直接进行连接请求数据了。但是用户的request可能并不是一个规范的http request请求，所以OKHttp会帮用户构建一个规范http request，比如添加一些头部信息，如Content-Length, Transfer-Encoding, User-Agent, Host, Connection, 和 Content-Type等等。除此之外，OKHttp的每一层都做了些什么？ 自定义应用拦截器 在这一层用户可以扩展针对请求数据和返回数据进行处理，比如打印返回数据的日志 重试/重定向应用拦截器 在这一层准备好后面进行网络连接的一些资源，比如连接池资源等。还提供了进行重试和重定向功能，因为网络环境不稳定情况下，已建立的连接可能会变成不可用状态，所以在这里会进行重试连接。还有一种情况就是，服务器返回了重定向码，在这一层将会新建一个request重新走一遍整个链 应用转网络的桥接器 在这里处理一些网络层特有的Header信息，比如Host属性。而Host属性包括Host、Connection的Keep-Alive、gzip透明压缩、User-Agent描述、Cookie策略等。 缓存拦截器 这一层主要是查询、存储和有效检查。存储，在走请求链获取Response时记录cache。查询，根据Request过滤，来查找Cache。有效性检查，判断Cache数据的各项指标是否达到条件。如果这层能查询到有效的cache，将会直接返回数据提高请求速度。 连接拦截器 到这一层为止，还没有进行真正的网络连接。它负责准备好请求、连接资源、传输工具、和连接，为连接前做好准备工作。 自定义网络拦截器 它和应用拦截器相比，它更能直接监测到在线网络请求的数据交换过程。更加适合做数据检测的自定义功能。可以在这层做的一些扩展有 一、模拟各种网络情况：断断续续的网络非常不好模拟，我们可以在网络拦截器层自由设定网络返回值和返回时间，辅助我们检查App在处理网络数据时的健壮性。 二、模拟数据辅助开发/测试：指向官网的地址重定向为指向开发测试网址，甚至直接mock返回数据，换掉在线数据，这样可以检测整个网络层的全部功能 在线网络请求拦截器 前面所有层，其实都是在为这一层做准备。真正进行连接，请求数据，返回数据的操作就在这一层 如何有效管理请求 请求有同步请求、异步请求 对直接思路就是直接放入Request中处理，但是OKHttp分出两个类来表示，拆分职责。分出一个类来对不同方式的请求进行管理。主要职责： 存储外界不断传入的 SyncCall 和AsyncCall，如果用户想取消则可以遍历所有的 call 进行 cancel 操作; 对于 SyncCall，由于它是即时运行的，因此Dispatcher 只需要在 SyncCall 运行前存储进来，在运行结束后移除即可； 对于 AsyncCall，Dispatcher 首先启动一个 ExecutorService，不断取出 AsyncCall 去进行执行，然后，我们设置最多执行的 request 数量为 64，如果已经有 64 个 request 在执行中，那么就将这个 asyncCall 存入等待区。 如何确保多个队列之间能顺畅地调度 对于多线程情况下的队列调度，其实就是数据移动和失败阻塞的这两个问题。 做了哪些优化OkHttp主要针对队列和线程池做了优化： 循环数组因为Dispatcher中的三个队列需要频繁出栈和入栈，所以采用了性能良好的循环数组ArrayDeque管理队列。 阻塞队列 因为Dispatcher自己用队列管理了排队的请求，所以Dispatcher中的线程池其实不需要缓存队列，那么这个线程池的任务其实是尽快地把元素转交给线程池中的io线程，所以采用了容量为0的阻塞队列SynchronousQueue，SynchronousQueue与普通队列不同，不是数据等线程，而是线程等数据，这样每次向SynchronousQueue里传入数据时，都会立即交给一个线程执行，这样可以提高数据得到处理的速度 控制线程数量 因为线程本身也会消耗资源，所以每个线程池都需要控制线程数量，OkHttp的线程池更进一步，会针对每个Host主机的请求（避免全都卡死在某个Host上），分别控制线程数上限（5个），具体方法就是遍历所有runningAsyncCall队列中的每个Call，查询每个Call的Host，并做计数。 我为什么要使用OKHttp？ volley是一个简单的异步http库，仅此而已。缺点是不支持同步，这点会限制开发模式；不能post大数据，所以不适合用来上传文件。 android-async-http。它是封装的httpClient，而android平台不推荐用HttpClient了，所以这个库已经不适合android平台了。 okhttp是高性能的http库，支持同步、异步，而且实现了spdy、http2、websocket协议，api很简洁易用，和volley一样实现了http协议的缓存。picasso就是利用okhttp的缓存机制实现其文件缓存，实现的很优雅，很正确，反例就是UIL（universal image loader），自己做的文件缓存，而且不遵守http缓存机制。 另一方面OKhttp还是用OKIO库，这个库相比原生IO更加高效： 它对数据进行了分块处理，这样在大数据IO的时候可以以块为单位进行IO，这可以提高IO的吞吐率 它对这些数据块使用链表进行管理，这可以仅通过移动“指针”就进行数据的管理，而不用真正去处理数据，而且对扩容来说也十分方便 对闲置的块进行管理，通过一个块池（SegmentPool）的管理，避免系统GC和申请byte时的zero-fill 其他的还有一些小细节上的优化，比如如果你把一个UTF-8的String转为ByteString，ByteString会保留一份对原来String的引用，这样当你下次 需要decode这个String时，程序通过保留的引用直接返回对应的String，从而避免了转码过程","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"EventBus跨进程思考","slug":"Android/跨进程EventBus","date":"2017-08-11T08:22:11.000Z","updated":"2018-03-05T16:44:51.174Z","comments":true,"path":"2017/08/11/Android/跨进程EventBus/","link":"","permalink":"https://kiddot.github.io/2017/08/11/Android/跨进程EventBus/","excerpt":"场景设想与分析 如果项目业务有一种场景需要使用EventBus进行跨进程通信，要怎么做？可能你会这样做↓","text":"场景设想与分析 如果项目业务有一种场景需要使用EventBus进行跨进程通信，要怎么做？可能你会这样做↓ ​ 主进程中创建一个本地服务LocalService，子进程中床架一个远程服务RemoteService，通信之前，先从业务模块通过EventBus post一个Event到本地LocalService，本地Service通过ServiceConnection与RemoteService绑定，本地LocalService在ServiceConnection的连接上将AIDL接口封装为一个Messenger，通过Messenger向RemoteService发送Message，而Event就是被包裹在Message中的Bundle里。 这样的做法缺点在于： 每个序列化的Object要放入Bundle中，Bundle对象再放入Message中，过程繁琐 每个Message都有what属性需要Handler在处理时判断 业务模块想要执行子进程的RemoteService的方法先通过EventBus post一个Object，在LocalService必须有相应的处理。 子进程中的RemoteService接收到了服务器发来的消息给业务模块，先要通过本地LocalService处理Message，再由EventBus把Message中的Object post出去 学习开源HermesEventBus思路​ 首先，区分出主进程和子进程。在做初始化工作的时候，在主进程中创建一个主进程服务作为注册，注销，发送事件等入口，通过注解记录下这个类的所有方法信息；在子进程做同样相似的工作，不同在于，在子进程中利用AIDL绑定了主进程中的Service与主进程通信，通信的载体可以是一个自己定义好的类。当完成绑定之后，在子进程中，通过代理获取主进程的服务来对子进程服务进行逐个登记。之后无论是子进程还是主进程，要进行发送事件，都通过主进程对子进程发送时间。 小结几种跨进程方法 BroadCastReceiver 由于面向整个系统注册的广播，跨进程消耗较大，性能不能保证。 ContentProvider 支持跨进程数据共享 AIDL 客户端调用AIDL接口是同步并且带返回结果的，如果执行时间较长，客户端的调用线程会一直等待。服务端执行AIDL接口是异步的，支持所有基本类型、AIDL接口、Parcelable、List、Map等类型的参数，实现起来繁琐。 Messenger 本质是AIDL通信，客户端发送Message后不带返回结果，服务端接收到Message是通过一个线程的Handler轮询MessageQueue处理的，因此处理Message是在同一线程。 HermesEventBus 本质也是AIDL通信，不需要自己实现绑定Service，发送事件也是不带返回结果的，使用简单。 Binder机制 Android跨进程通信实现的核心，AIDL就是基于Binder机制实现的，其中transact方法是客户端向服务端发送消息，onTransact方法是客户端接收服务端的消息。","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"WebView总结","slug":"Android/WebView总结","date":"2017-08-09T08:22:11.000Z","updated":"2018-03-05T16:44:12.501Z","comments":true,"path":"2017/08/09/Android/WebView总结/","link":"","permalink":"https://kiddot.github.io/2017/08/09/Android/WebView总结/","excerpt":"WebView总结","text":"WebView总结 WebView的基本使用 权限 12345678910 &lt;uses-permission android:name=\"android.permission.INTERNET\" /&gt;&lt;!--另外附上一些可能会用到的权限：--&gt;&lt;uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" /&gt;&lt;!--获取网络状态权限（情景：WebView联网前，应检查当前网络状态）--&gt;&lt;uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\"/&gt;&lt;!--通过WiFi或移动基站的方式获取用户错略的经纬度信息--&gt;&lt;uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\" /&gt;&lt;!--通过GPS芯片接收卫星的定位信息权限（情景：结合HTML5使用Geolocation API获取位置时）--&gt;&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" /&gt;&lt;!--读写存储权限（情景：拍照/选择图片，涉及图片读取、编辑、压缩等功能时）--&gt; 常用配置 12345678910111213141516171819202122WebSettings mWebSetting = mWebView.getSettings(); //获取WebSettingsetJavaScriptEnabled(true);//让WebView支持JavaScriptsetDomStorageEnabled(true);//启用H5 DOM API （默认false）setDatabaseEnabled(true);//启用数据库api（默认false）可结合 setDatabasePath 设置路径setCacheMode(WebSettings.LOAD_DEFAULT)//设置缓存模式setAppCacheEnabled(true);//启用应用缓存（默认false）可结合 setAppCachePath 设置缓存路径setAppCacheMaxSize()//已过时，高版本API上，系统会自行分配setPluginsEnabled(true); //设置插件支持setRenderPriority(RenderPriority.HIGH); //提高渲染的优先级setUseWideViewPort(true); //将图片调整到适合webview的大小setLoadWithOverviewMode(true); // 缩放至屏幕的大小setSupportZoom(true); //支持缩放，默认为truesetBuiltInZoomControls(true); //设置内置的缩放控件（若SupportZoom为false，该设置项无效）setDisplayZoomControls(false); //隐藏原生的缩放控件setLayoutAlgorithm(LayoutAlgorithm.SINGLE_COLUMN); //支持内容重新布局supportMultipleWindows(); //支持多窗口setCacheMode(WebSettings.LOAD_CACHE_ELSE_NETWORK); //关闭webview中缓存setAllowFileAccess(true); //设置可以访问文件setNeedInitialFocus(true); //当webview调用requestFocus时为webview设置节点setJavaScriptCanOpenWindowsAutomatically(true); //支持通过JS打开新窗口setLoadsImagesAutomatically(true); //自动加载图片setDefaultTextEncodingName(\"utf-8\");//设置编码格式 加载页面的三种方式 123456789//网页private static final String URL_NET = \"http://www.google.com\"; // 记得加 \"http://\"//assets 中的 html 资源private static final String URL_LOCAL =\"file:///android_asset/xxx.html路径\"; //SD 卡中的 html 资源private static final String URL_SD_CARD =\"content://com.android.htmlfileprovider/mnt/sdcard/xxx.html\"; mWebView.loadUrl(URL_NET);mWebView.loadUrl(URL_LOCAL);mWebView.loadUrl(URL_SD_CARD); WebViewClient &amp; WebChromeClientWebViewClient主要用于帮助webview处理各种通知和请求时间 shouldOverrideUrlLoading 加载时调用，可捕获url onPageStart 开始加载时调用 onPageFinish 加载完成时调用 onReceiveError 接收到错误信息时调用，通常用来处理404等加载错误， 但是在低于API23时，该方法失效，可能是大于API23不再接受网页连接错误，而是WebView自身的错误 WebChromeClient用于辅助WebView处理JS的对话框，网站图标，位置，加载进度等信息 onProgressChanged 加载进度 onReceivedTitle、onReceivedIcon 获取网页标题和图标 WebView与JS的交互 前提条件：setJavaScriptEnabled(true) 调用JS方法 mWebView.loadUrl(“javascript: 方法名(‘“+参数+”‘)”); js调用android方法 123456789101112131415 //使用方法 mWebView.addJavascriptInterface(MethodObject,\"name\"); //还需要写一个方法类 class MethodObject extends Object &#123; //无参函数，js中通过：var str = window.name.HtmlcallJava(); 获取到 @JavascriptInterface public String HtmlcallJava() &#123; return \"Html call Java\"; &#125; //有参函数，js中通过：window.jsObj.HtmlcallJava2(\"IT-homer blog\"); @JavascriptInterface public String HtmlcallJava2(final String param) &#123; return \"Html call Java : \" + param; &#125;&#125; WebView优化1.页面加载速度 影响页面加载速度的因素有非常多，每次加载的过程中都会有较多的网络请求，除了 web 页面自身的 URL 请求，还会有 web 页面外部引用的JS、CSS、字体、图片等等都是个独立的 http 请求。这些请求都是串行的，这些请求加上浏览器的解析、渲染时间就会导致 WebView 整体加载时间变长 选择合适的 WebView 缓存（二次加载速度优化） WebView缓存分为：页面缓存和数据缓存。页面缓存指加载网页时，对页面或资源数据的缓存（一般使用RE管理器进入目录： “/data/data/(packageName)/cache/org.chromium.android_webview“可看 ）。数据缓存又分为 AppCache 与 DOM Storage 。AppCache可以有选择地缓存我们所想要缓存的东西；DOM Storage 则是HTML5的一个缓存机制，常用于存储简单的表单数据 WebView的缓存模式 | LOAD_CACHE_ONLY | 不使用网络，只读取本地缓存数据 || :———————: | :————————————–: || LOAD_DEFAULT | 根据cache-control决定是否从网络上取数据 || LOAD_CACHE_NORMAL | API level 17中已经废弃, 从API level 11开始作用同LOAD_DEFAULT模式 || LOAD_NO_CACHE | 不使用缓存，只从网络获取数据 || LOAD_CACHE_ELSE_NETWORK | 只要本地有，无论是否过期，或者no-cache，都使用缓存中的数据 | ​ 缓存的数据转移到SD卡 重写Context 类中的getCacheDir方法即可 123456789101112131415161718192021222324252627@Override public void onCreate() &#123; super.onCreate(); if (Environment.MEDIA_MOUNTED.equals(Environment.getExternalStorageState()))&#123; File externalStorageDir = Environment.getExternalStorageDirectory(); if (externalStorageDir != null)&#123; // &#123;SD_PATH&#125;/Android/data/ extStorageAppBasePath = new File(externalStorageDir.getAbsolutePath() + File.separator +\"Android\" + File.separator + \"data\"+File.separator + getPackageName()); Log.i(TAG,extStorageAppBasePath+\"=====\"); &#125; if (extStorageAppBasePath != null)&#123; extStorageAppCachePath = new File(extStorageAppBasePath.getAbsolutePath()+File.separator + \"webViewCache\"); Log.d(\"extStorageAppCachePath\",\"extStorageAppCachePath = \"+extStorageAppCachePath); boolean isCachePathAvailable = true; if (!extStorageAppCachePath.exists())&#123; isCachePathAvailable = extStorageAppCachePath.mkdirs(); if (!isCachePathAvailable)&#123; extStorageAppCachePath = null; &#125; &#125; &#125; &#125; &#125; @Override public File getCacheDir() { if (extStorageAppCachePath != null){ return extStorageAppCachePath; }else{ return super.getCacheDir(); } } 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859 ​- **资源预加载**（首次速度优化）&gt; 缓存技术，能优化二次启动 WebView 的加载速度，那首次加载 H5 页面的速度该怎么优化?一种思路就是外部依赖的 JS、CSS、图片等资源先提前在wifi环境下先预载好WebView mWebView = (WebView) findViewById(R.id.webview);mWebView.setWebViewClient(new WebViewClient() &#123; @Override public WebResourceResponse shouldInterceptRequest(WebView webView, final String url) &#123; WebResourceResponse response = null; // 检查该资源是否已经提前下载完成。我采用的策略是在应用启动时，用户在 wifi 的网络环境下 // 提前下载 H5 页面需要的资源。 boolean resDown = JSHelper.isURLDownValid(url); if (resDown) &#123; jsStr = JsjjJSHelper.getResInputStream(url); if (url.endsWith(&quot;.png&quot;)) &#123; response = getWebResourceResponse(url, &quot;image/png&quot;, &quot;.png&quot;); &#125; else if (url.endsWith(&quot;.gif&quot;)) &#123; response = getWebResourceResponse(url, &quot;image/gif&quot;, &quot;.gif&quot;); &#125; else if (url.endsWith(&quot;.jpg&quot;)) &#123; response = getWebResourceResponse(url, &quot;image/jepg&quot;, &quot;.jpg&quot;); &#125; else if (url.endsWith(&quot;.jepg&quot;)) &#123; response = getWebResourceResponse(url, &quot;image/jepg&quot;, &quot;.jepg&quot;); &#125; else if (url.endsWith(&quot;.js&quot;) &amp;&amp; jsStr != null) &#123; response = getWebResourceResponse(&quot;text/javascript&quot;, &quot;UTF-8&quot;, &quot;.js&quot;); &#125; else if (url.endsWith(&quot;.css&quot;) &amp;&amp; jsStr != null) &#123; response = getWebResourceResponse(&quot;text/css&quot;, &quot;UTF-8&quot;, &quot;.css&quot;); &#125; else if (url.endsWith(&quot;.html&quot;) &amp;&amp; jsStr != null) &#123; response = getWebResourceResponse(&quot;text/html&quot;, &quot;UTF-8&quot;, &quot;.html&quot;); &#125; &#125; // 若 response 返回为 null , WebView 会自行请求网络加载资源。 return response; &#125; &#125;);private WebResourceResponse getWebResourceResponse(String url, String mime, String style) &#123; WebResourceResponse response = null; try &#123; response = new WebResourceResponse(mime, &quot;UTF-8&quot;, new FileInputStream(new File(getJSPath() + TPMD5.md5String(url) + style))); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; return response; &#125;public String getJsjjJSPath() &#123; String splashTargetPath = JarEnv.sApplicationContext.getFilesDir().getPath() + &quot;/JS&quot;; if (!TPFileSysUtil.isDirFileExist(splashTargetPath)) &#123; TPFileSysUtil.createDir(splashTargetPath); &#125; return splashTargetPath + &quot;/&quot;; &#125; JS 脚本本地化（首次加载优化） ​比预加载更粗暴的优化方法是直接将常用的 JS 脚本本地化，直接打包放入 apk 中。比如 H5 页面获取用户信息，设置标题等通用方法，就可以直接写入一个 JS 文件，放入 asserts 文件夹，在 WebView 调用了onPageFinished() 方法后进行加载。但是，该 JS 文件中需要写入一个 JS 文件载入完毕的事件 使用第三方WebView内核 Android 4.4 版本 Google 使用了Chromium 替代 Webkit 作为 WebView 内核,但是我们可以使用腾讯集成的SDK，就可以使用X5内核了，更换第三方内核速度之后各方面速度都会有较大的提升 2.内存泄露&amp;&amp;内存优化 ​ WebView解析网页时会申请Native堆内存用于保存页面元素，当页面较复杂时会有很大的内存占用。为了打开新页面时，为了能快速回退，之前页面占用的内存也不会释放。有时浏览十几个网页，都会占用几百兆的内存。这样加载网页较多时，会导致系统不堪重负，最终强制关闭应用，也就是出现应用闪退或重启。 ​ 由于占用的都是Native堆内存，所以实际占用的内存大小不会显示在常用的DDMS Heap工具中（这里看到的只是Java虚拟机分配的内存，一般即使Native堆内存已经占用了几百兆，这里显示的还只是几兆或十几兆）。只有使用adb shell中的一些命令比如dumpsys meminfo 包名，或者在程序中使用Debug.getNativeHeapSize()才能看到。 ​ 由于WebView的一个BUG，即使它所在的Activity(或者Service)结束也就是onDestroy()之后，或者直接调用WebView.destroy()之后，它所占用这些内存也不会被释放。 解决这个问题最直接的方法是：把使用了WebView的Activity(或者Service)放在单独的进程里。然后在检测到应用占用内存过大有可能被系统干掉或者它所在的Activity(或者Service)结束后，调用System.exit(0)，主动Kill掉进程。由于系统的内存分配是以进程为准的，进程关闭后，系统会自动回收所有内存。","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"Android的渲染机制","slug":"Android/Android的渲染机制","date":"2017-08-03T08:22:11.000Z","updated":"2018-03-05T16:43:34.540Z","comments":true,"path":"2017/08/03/Android/Android的渲染机制/","link":"","permalink":"https://kiddot.github.io/2017/08/03/Android/Android的渲染机制/","excerpt":"Android的渲染机制知识储备","text":"Android的渲染机制知识储备 CPU ：中央处理器,它集成了运算,缓冲,控制等单元,包括绘图功能.CPU将对象处理为多维图形,纹理 GPU ： 一个类似于CPU的专门用来处理Graphics的处理器, 作用用来帮助加快栅格化操作。也具备缓存的数据 OpenGL ES ： 手持嵌入式设备的3DAPI,跨平台的、功能完善的2D和3D图形应用程序接口API DisplayList ： 在Android把XML布局文件转换成GPU能够识别并绘制的对象。这个操作是在DisplayList的帮助下完成的。DisplayList持有所有将要交给GPU绘制到屏幕上的数据信息。 栅格化 ：是 将图片等矢量资源,转化为一格格像素点的像素图,显示到屏幕上,过程图如下. 垂直同步VSYNC : 让显卡的运算和显示器刷新率一致以稳定输出的画面质量。它告知GPU在载入新帧之前，要等待屏幕绘制完成前一帧。 Refresh Rate ： 屏幕一秒内刷新屏幕的次数,由硬件决定,例如60Hz Frame Rate ： GPU一秒绘制操作的帧数,单位是30fps 渲染机制分析渲染流程线 UI对象 CPU处理为多维图形，纹理 通过OpenGL ES接口调用GPU GPU对图进行光栅化 硬件时钟，垂直同步 投射到屏幕 流程如下图： 渲染时间线 Android系统每隔16ms发出垂直信号，触发对UI的渲染，如果每次渲染成功，这样就能够达到流畅的画面所需要的60fps，这意味这计算渲染的大多数操作都必须在16ms内完成。 正常情况 渲染超时，即超过16ms 当一帧画面渲染超过16ms的时候，垂直同步机制会让显示器硬件等待GPU完成栅格化渲染操作，这样会让一帧画面多停留16ms，甚至更多，这样就造成了画面卡顿的视觉效果。 渲染时会出现的问题GPU过度绘制 GPU的绘制过程,就跟刷墙一样,一层层的进行,16ms刷一次.这样就会造成,图层覆盖的现象,即无用的图层还被绘制在底层,造成不必要的浪费. 过度绘制查看工具 在手机端的开发者选项里,有OverDraw监测工具,调试GPU过度绘制工具,其中颜色代表渲染的图层情况,分别代表1层,2层,3层,4层覆盖. 计算渲染的耗时 任何时候View中的绘制内容发生变化时，都会重新执行创建DisplayList，渲染DisplayList，更新到屏幕上等一系列操作。这个流程的表现性能取决于你的View的复杂程度，View的状态变化以及渲染管道的执行性能。 当View的大小发生改变，DisplayList就会重新创建，然后再渲染，而当View发生位移，则DisplayList不会重新创建，而是执行重新渲染的操作。 当View过于复杂，操作又过于复杂，就会计算渲染时间超过16ms，产生卡顿问题。 如何优化Android系统本身的优化在Android里面那些由主题所提供的资源，例如Bitmaps，Drawables都是一起打包到统一的Texture纹理当中，然后再传递到 GPU里面，这意味着每次你需要使用这些资源的时候，都是直接从纹理里面进行获取渲染的。 程序员自身需要做的优化 扁平化处理,防止过度绘制OverDraw 每个layout最外层的父容器是否需要？ 多余的布局，比如LinearLayout等，会让GPU多渲染一层图，所以能省去就省去。 布局层级优化 查看自己的布局，深的层级，能减少层级的情况尽量减少。 Hierarchy Viewer工具这是查看耗时的工具和布局树的深度的工具 利用它，可以查看自己布局树，尽量减少树的深度，减少每个View的渲染时间 图片选择Android界面能用png尽量用png。因为32位png颜色过度平滑且支持透明。jpg是像素化压缩过的图片，质量已经下降了，不适合再拿来做9path的按钮和平铺拉伸的空间。 对于颜色繁杂的，照片墙纸之类的图片（应用的启动画面喜欢搞这种），那用jpg是最好不过了，这种图片压缩前压缩后肉眼分辨几乎不计，如果保存成png体积将是jpg的几倍甚至几十倍，严重浪费体积。 清理不必要的背景 当背景无法避免，尽量使用Color.TRANSPARENT 因为透明色Color.TRANSPARENT是不会被渲染的,他是透明的. 优化自定义View的计算View中的方法OnMeasure,OnLayout,OnDraw.在我们自定义View起到了决定作用,要学会研究其中的优化方法. 比如学会裁剪掉View的覆盖部分,增加cpu的计算量,来优化GPU的渲染","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"Amigo的替换DEX修复思路","slug":"Android/Amigo的替换DEX修复思路","date":"2017-08-01T08:22:11.000Z","updated":"2018-03-05T16:43:16.460Z","comments":true,"path":"2017/08/01/Android/Amigo的替换DEX修复思路/","link":"","permalink":"https://kiddot.github.io/2017/08/01/Android/Amigo的替换DEX修复思路/","excerpt":"Amigo的替换DEX修复思路知识储备Dex ​ APP有自己的类，这些类保存在APK的dex文件里面，所以APP启动的时候，也会创建一个自己的ClassLoader实例，用于加载自己dex文件中的类。","text":"Amigo的替换DEX修复思路知识储备Dex ​ APP有自己的类，这些类保存在APK的dex文件里面，所以APP启动的时候，也会创建一个自己的ClassLoader实例，用于加载自己dex文件中的类。 ClassLoader ​ 一个运行的Android应用至少有2个ClassLoader。 ​ 一个是BootClassLoader（系统启动的时候创建的），另一个是PathClassLoader（应用启动时创建的，用于加载“/data/app/me.kaede.anroidclassloadersample-1/base.apk”里面的类）。 android中加载类一般使用的是PathClassLoader和DexClassLoader ​ 对于PathClassLoader，Android是使用这个类作为其系统类和应用类的加载器。并且对于这个类呢，只能去加载已经安装到Android系统中的apk文件。对于DexClassLoader，可以用来从.jar和.apk类型的文件内部加载classes.dex文件。可以用来执行非安装的程序代码。 ​ 一个ClassLoader可以包含多个dex文件，每个dex文件是一个Element，多个dex文件排列成一个有序的数组dexElements，当找类的时候，会按顺序遍历dex文件，然后从当前遍历的dex文件中找类，如果找类则返回，如果找不到从下一个dex文件继续查找。(来自：安卓App热补丁动态修复技术介绍) 整体流程一、检查补丁包​ 检查是否有补丁包，并且签名正确，如果正确，则通过检验校验和是否与之前的检验和相同，不同则为检测到新的补丁包。 二、释放Apk​ 释放 Dex 到指定目录，即解压apk并且输出到指定目录 三、加载、合并Dex​ 将补丁包中每个 dex 对应的 Element 对象拿出来，之后组成新的 Element[]。 四、替换​ 通过反射注入到当前的ClassLoader,将现有的 Element[] 数组替换掉。 ​ 在 QZone 的实现方案中，他们是通过将新的 dex 插到 Element[] 数组的第一个位置，这样就会先加载新的 dex ，微信的方案是下发一个 DiffDex，然后在运行时与旧的 dex 合成一个新的 dex。但是 Amigo 是下发一个完整的 dex直接替换掉了原来的 dex。与其他的方案相比，Amigo 因为直接替换原来的 dex ,兼容性更好，能够支持修复的方面也更多。","categories":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://kiddot.github.io/tags/Android/"}]},{"title":"Cookie与Session的作用与原理","slug":"计网/Cookie与Session的作用与原理","date":"2017-07-30T04:26:11.000Z","updated":"2018-03-05T16:41:15.917Z","comments":true,"path":"2017/07/30/计网/Cookie与Session的作用与原理/","link":"","permalink":"https://kiddot.github.io/2017/07/30/计网/Cookie与Session的作用与原理/","excerpt":"Cookie与Session的作用与原理","text":"Cookie与Session的作用与原理 一、Cookie详解 （1）简介 因为HTTP协议是无状态的，即服务器不知道用户上一次做了什么，这严重阻碍了交互式Web应用程序的实现。在典型的网上购物场景中，用户浏览了几个页面，买了一盒饼干和两饮料。最后结帐时，由于HTTP的无状态性，不通过额外的手段，服务器并不知道用户到底买了什么。为了做到这点，就需要使用到Cookie了。服务器可以设置或读取Cookies中包含信息，借此维护用户跟服务器会话中的状态。 Cookie（复数形态：Cookies），是指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）。 Cookie是由服务端生成的，发送给客户端（通常是浏览器）的。Cookie总是保存在客户端中，按在客户端中的存储位置，可分为内存Cookie和硬盘Cookie： 内存Cookie由浏览器维护，保存在内存中，浏览器关闭后就消失了，其存在时间是短暂的。 硬盘Cookie保存在硬盘里，有一个过期时间，除非用户手工清理或到了过期时间，硬盘Cookie不会被删除，其存在时间是长期的。所以，按存在时间，可分为非持久Cookie和持久Cookie。 （2）工作原理 1、创建Cookie 当用户第一次浏览某个使用Cookie的网站时，该网站的服务器就进行如下工作： ①该用户生成一个唯一的识别码（Cookie id），创建一个Cookie对象； ②默认情况下它是一个会话级别的cookie，存储在浏览器的内存中，用户退出浏览器之后被删除。如果网站希望浏览器将该Cookie存储在磁盘上，则需要设置最大时效（maxAge），并给出一个以秒为单位的时间（将最大时效设为0则是命令浏览器删除该Cookie）； ③将Cookie放入到HTTP响应报头，将Cookie插入到一个 Set-Cookie HTTP请求报头中。 ④发送该HTTP响应报文。 2、设置存储Cookie 浏览器收到该响应报文之后，根据报文头里的Set-Cookied特殊的指示，生成相应的Cookie，保存在客户端。该Cookie里面记录着用户当前的信息。 3、发送Cookie 当用户再次访问该网站时，浏览器首先检查所有存储的Cookies，如果某个存在该网站的Cookie（即该Cookie所声明的作用范围大于等于将要请求的资源），则把该cookie附在请求资源的HTTP请求头上发送给服务器。 4、读取Cookie 服务器接收到用户的HTTP请求报文之后，从报文头获取到该用户的Cookie，从里面找到所需要的东西。 （3）作用 Cookie的根本作用就是在客户端存储用户访问网站的一些信息。典型的应用有： 1、记住密码，下次自动登录。 2、购物车功能。 3、记录用户浏览数据，进行商品（广告）推荐。 （4）缺陷 ①Cookie会被附加在每个HTTP请求中，所以无形中增加了流量。 ②由于在HTTP请求中的Cookie是明文传递的，所以安全性成问题。（除非用HTTPS） ③Cookie的大小限制在4KB左右。对于复杂的存储需求来说是不够用的。 二、Session详解 （1）简介 Session代表服务器与浏览器的一次会话过程，这个过程是连续的，也可以时断时续的。Session是一种服务器端的机制，Session 对象用来存储特定用户会话所需的信息。 Session由服务端生成，保存在服务器的内存、缓存、硬盘或数据库中。 （2）工作原理 1、创建Session 当用户访问到一个服务器，如果服务器启用Session，服务器就要为该用户创建一个SESSION，在创建这个SESSION的时候，服务器首先检查这个用户发来的请求里是否包含了一个SESSION ID，如果包含了一个SESSION ID则说明之前该用户已经登陆过并为此用户创建过SESSION，那服务器就按照这个SESSION ID把这个SESSION在服务器的内存中查找出来（如果查找不到，就有可能为他新创建一个），如果客户端请求里不包含有SESSION ID，则为该客户端创建一个SESSION并生成一个与此SESSION相关的SESSION ID。这个SESSION ID是唯一的、不重复的、不容易找到规律的字符串，这个SESSION ID将被在本次响应中返回到客户端保存，而保存这个SESSION ID的正是COOKIE，这样在交互过程中浏览器可以自动的按照规则把这个标识发送给服务器。 2、使用Session 我们知道在IE中，我们可以在工具的Internet选项中把Cookie禁止，那么会不会出现把客户端的Cookie禁止了，那么SESSIONID就无法再用了呢？找了一些资料说明，可以有其他机制在COOKIE被禁止时仍然能够把Session id传递回服务器。 经常被使用的一种技术叫做URL重写，就是把Session id直接附加在URL路径的后面一种是作为URL路径的附加信息,表现形式为： http://…./xxx;jSession=ByOK3vjFD75aPnrF7C2HmdnV6QZcEbzWoWiBYEnLerjQ99zWpBng!-145788764； 另一种是作为查询字符串附加在URL后面，表现形式为： http://…../xxx?jSession=ByOK3vjFD75aPnrF7C2HmdnV6QZcEbzWoWiBYEnLerjQ99zWpBng!-145788764 还有一种就是表单隐藏字段。就是服务器会自动修改表单，添加一个隐藏字段，以便在表单提交时能够把Session id传递回服务器。 （3）作用 Session的根本作用就是在服务端存储用户和服务器会话的一些信息。典型的应用有： 1、判断用户是否登录。 2、购物车功能。 三、Cookie和Session的区别 1、存放位置不同 Cookie保存在客户端，Session保存在服务端。 2 、存取方式的不同 Cookie中只能保管ASCII字符串，假如需求存取Unicode字符或者二进制数据，需求先进行编码。Cookie中也不能直接存取Java对象。若要存储略微复杂的信息，运用Cookie是比拟艰难的。 而Session中能够存取任何类型的数据，包括而不限于String、Integer、List、Map等。Session中也能够直接保管Java Bean乃至任何Java类，对象等，运用起来十分便当。能够把Session看做是一个Java容器类。 3、安全性（隐私策略）的不同 Cookie存储在浏览器中，对客户端是可见的，客户端的一些程序可能会窥探、复制以至修正Cookie中的内容。而Session存储在服务器上，对客户端是透明的，不存在敏感信息泄露的风险。 假如选用Cookie，比较好的方法是，敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。 4、有效期上的不同 只需要设置Cookie的过期时间属性为一个很大很大的数字，Cookie就可以在浏览器保存很长时间。 由于Session依赖于名为JSESSIONID的Cookie，而Cookie JSESSIONID的过期时间默许为–1，只需关闭了浏览器（一次会话结束），该Session就会失效。 5、对服务器造成的压力不同 Session是保管在服务器端的，每个用户都会产生一个Session。假如并发访问的用户十分多，会产生十分多的Session，耗费大量的内存。而Cookie保管在客户端，不占用服务器资源。假如并发阅读的用户十分多，Cookie是很好的选择。 6、 跨域支持上的不同 Cookie支持跨域名访问，例如将domain属性设置为“.baidu.com”，则以“.baidu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。","categories":[{"name":"计网","slug":"计网","permalink":"https://kiddot.github.io/categories/计网/"}],"tags":[{"name":"计网","slug":"计网","permalink":"https://kiddot.github.io/tags/计网/"}]},{"title":"计网总结","slug":"计网/计网总结","date":"2017-07-23T04:26:11.000Z","updated":"2018-03-05T16:42:12.422Z","comments":true,"path":"2017/07/23/计网/计网总结/","link":"","permalink":"https://kiddot.github.io/2017/07/23/计网/计网总结/","excerpt":"计网总结物理层","text":"计网总结物理层 数据链路层三个基本问题 封装成帧 透明传输 差错检测 PPP帧与Mac帧的区别 ppp属于广域网范畴，MAC是局域网范畴，按实际情况和环境就选用不同的协议，ppp支持的网络结构只能是点对点，mac支持多点对多点。 以太网中用mac，远程的话就用ppp（如ADSL拨号，就是基于ppp的）。 ppp是点到点协议 ,逻辑上相连的就一台设备，因此不需要寻址, 目标地址为广播地址, PPP中前6个字节就是目标地址。 网络层 提供主机间的逻辑通信 ARPARP是解决同一个局域网中主机或路由器IP地址到硬件地址的映射问题。 每台主机都有一个ARP高速缓存，里面存放着一个从IP地址到硬件地址的映射表，并且动态更新 步骤： ARP进程在局域网上广播发送一个ARP请求分组。（附带上自己的IP地址和硬件地址以及目的地址） 在局域网上所有主机上运行ARP进程都会收到这个ARP分组 主机B的IP地址和ARP请求分组中要查询的一致，就收下这个请求分组，并且向主机A发送ARP响应分组 主机A收到主机B的ARP响应分组后，在自己的ARP高速缓存中写入主机B的IP地址到硬件地址的映射 ICMPICMP报文是装在IP数据报中的，作为其中的数据部分。 应用： ping tracerouter 路由选择协议内部网关协议IGPRIP：基于距离向量 优点： 配置简单 开销小 缺点 大量广播。RIP向所有邻居每隔30秒广播一次完整的路由表,将占用宝贵的带宽资源,在较慢的广域网链路上尤其有问题｡ 没有成本概念｡RIP没有网络延迟和链路成本的概念｡当采用RIP时,路由/转发的决定只是基于跳线,这样,很容易导致无法选择最佳路由｡例如,一条链路拥有较高的带宽,但是,跳数较多,从而不能被选择｡ 支持的网络规模有限｡由于RIP路由协议最多只支持16个步跳,当超过该跳数时,网络将认为无法到达｡因此,RIP只能适用于规模较少的网络｡ OSPF:最短路径优先 优点： 快速收敛｡OSPF是真正的LOOP- FREE(无路由自环)路由协议｡源自其算法本身——链路状态及最短路径树算法,OSPF收敛速度快,能够在最短的时间内将路由变化传递到整个自治系统｡ 区域划分｡提出区域(Area)划分的概念,将自治系统划分为不同区域后,通过区域之间的对路由信息的摘要,大大减少了需传递的路由信息数量,也使得路由信息不会随网络规模的扩大而急剧膨胀｡ 开销控制｡将协议自身的开销控制到最小｡用于发现和维护邻居关系的是定期发送的不含路由信息的hello报文,非常短小｡包含路由信息的报文是触发更新的机制,而且只有在路由变化时才会发送｡但为了增强协议的健壮性,每1800秒全部重发一次｡ 安全性高｡良好的安全性,OSPF支持基于接口的明文及MD5 验证｡ 缺点： 配置相对复杂｡由于网络区域划分和网络属性的复杂性,需要网络分析员有较高的网络知识水平才能配置和管理OSPF网络｡ 路由负载均衡能力较弱｡OSPF虽然能根据接口的速率､连接可靠性等信息,自动生成接口路由优先级,但在通往同一目的的不同优先级路由中,OSPF只选择优先级较高的转发,不同优先级的路由中,不能实现负载分担｡只有相同优先级的,才能达到负载均衡的目的 外部网关协议EGPBGP 高度抽象，把AS自治系统抽象成一个路由，它就是一个基于路径向量路由选择协议。每个AS系统会有一个或者多个路由器充当发言人，根据收到的路由信息拓扑出AS的树型结构连通图 NAT运输层 提供应用进程间端到端的逻辑通信 UDP 无连接，不可靠，面向报文 无拥塞控制，支持一对一，一对多，多对多，多对一，首部开销小 检验和把首部和数据部分一起都检验 UDP丢包丢包的主要原因 接收端处理时间过长导致丢包：调用recv方法接收端收到数据后，处理数据花了一些时间，处理完后再次调用recv方法，在这二次调用间隔里，发过来的包可能丢失。对于这种情况可以修改接收端，将包接收后存入一个缓冲区，然后迅速返回继续recv. 发送的包较大，超过接受者缓存导致丢包：包超过mtu size数倍，几个大的udp包可能会超过接收者的缓冲，导致丢包 发送的包频率太快：虽然每个包的大小都小于mtu size 但是频率太快 解决方案 可以修改接收端，将包接收后存入一个缓冲区 对于超过缓存大小的包，可以选择直接接受 通过流量控制 TCP 面向连接，可靠 面向字节流 全双工通信 TCP粘包拆包 粘包拆包出现的原因 ： 因为TCP面向字节流传输的，发送端需要等缓冲区满才发送出去，造成粘包。接收方不及时接收缓冲区的包，造成多个包接收 非工程性的解决方法： 利用TCP提供了强制数据立即传送的操作指令push缺点：但它关闭了优化算法，降低了网络发送效率，影响应用程序的性能 精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据缺点：只能减少出现粘包的可能性，但并不能完全避免粘包。因为当发送频率较高时，或由于网络突发可能使某个时间段数据包到达接收方较快，接收方还是有可能来不及接收，从而导致粘包。 两次send函数之间添加 sleep函数缺点：会降低数据传输效率 工程型的解决方法： 1.消息定长，例如每个报文的大小为固定长度200字节,如果不够，空位补空格； 2.在包尾增加回车换行符进行分割（模仿帧的设计方式）； 3.将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度）的字段，通常设计思路是消息头的第一个字段用int来表示消息的总长度；（在pushsdk中使用此种方式利用了Netty中的LengthFieldBasedFrameDecoder解码器实现) 4.更复杂的应用层协议； 添加标志字段，在每次发送数据是添加标记字段：A： =&gt;size 标记数据长度的方式 B：特定标记字段标记数据的结尾（模仿帧的设计方式）＝&gt;结束符的方式 定义应用层的数据通讯协议 ：=&gt;如果数据按照一定的方式存储或着优加密的需求， 可以通过自己定制 数据通讯协议对数据封装，并实现自己的数据 封包｜ 拆包函数。 为了解决TCP粘包拆包的问题，Netty默认提供了多种编码器来处理 根据MTU拆分成多个数据报http://blog.csdn.net/yusiguyuan/article/details/22782943 （TCP层的分段和IP层的分片之间的关系 &amp; MTU和MSS之间的关系） 三次握手 在客户端和服务端都创建好传输控制模块TCB的条件下： 客户端向服务端发出连接请求报文段 服务端接受到连接请求报文段后，同意建立连接就返回一个确认报文段 客户端的TCP客户进程收到确认后，还需要向服务端发出确认报文段 ps:传输控制模块TCB:存储了每一个连接中的一些重要信息，比如TCP连接表，重传队列指针，当前发送的序列号等等。 为什么是三次握手？ 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误 会有这样的一种情况：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。”。主要目的防止server端一直等待，浪费资源。 四次挥手 1.客户端发送连接释放报文段，并且停止发送数据，主动关闭TCP连接，然后进入等待状态，等待服务端的确认 2.服务端收到连接释放报文段之后，立即发送确认报文段，然后进入关闭等待状态。并且TCP服务器进程会通知高层应用进程。这个时候，客户端到服务端这个方向的连接就处于半关闭状态。 客户端收到确认报文段之后，等待服务端发出连接释放报文段 3.若服务端没有要向客户端发送的数据之后，应用进程会通知TCP释放连接。这个时候，服务端会发出连接释放报文段 4.客户端收到连接释放报文段之后，会返回一个确认报文段。然后自己进入时间等待状态，即等待一个时间才会最终进入关闭状态。 为什么需要等待一个时间？ 一、为了保证客户端最后发送的一个报文段能够到达服务端 二、防止已失效的连接请求报文段出现在本连接中 服务端只要收到了客户端最后一次的确认报文段就会进入关闭状态，结束这次TCP连接 为什么需要四次挥手？ 确保数据能够完成传输。 假设服务端收到连接释放报文段之后马上就释放连接，那么服务端还存在数据需要发送给客户端的话，就不能保证这些数据能够发送到客户端身上。因此需要等待服务端把数据发送完之后，由服务端发起一条连接释放报文段，也就是第三次挥手，最后由客户端发起确认，也就是第四次挥手，因此需要四次挥手才能确保数据能够完成完整的传输。 可靠传输的实现TCP可靠传输的实现 TCP为了提供可靠传输： （1）首先，采用三次握手来建立TCP连接，四次握手来释放TCP连接，从而保证建立的传输信道是可靠的。 （2）其次，TCP采用了连续ARQ协议（回退N，Go-back-N；超时自动重传）来保证数据传输的正确性，使用滑动窗口协议来保证接方能够及时处理所接收到的数据，进行流量控制。 （3）最后，TCP使用慢开始、拥塞避免、快重传和快恢复来进行拥塞控制，避免网络拥塞。 以字节为单位的滑动窗口 缓存机制 发送缓存用来暂时存放：1.发送应用程序传送给发送方TCP准备的数据2.TCP已发送但尚未收到确认的数据。 接收缓存用来暂时存放：1.按序到达的，但尚未被接收应用程序读取的数据。2.未按序到达的数据。 超时重传的时间选择 TCP流量控制原理 所谓流量控制就是让发送发送速率不要过快，让接收方来得及接收。利用滑动窗口机制就可以实施流量控制。 ​ 原理这就是运用TCP报文段中的窗口大小字段来控制，发送方的发送窗口不可以大于接收方发回的窗口大小。 ​ 考虑一种特殊的情况，就是接收方若没有缓存足够使用，就会发送零窗口大小的报文，此时发送放将发送窗口设置为0，停止发送数据。之后接收方有足够的缓存，发送了非零窗口大小的报文，但是这个报文在中途丢失的，那么发送方的发送窗口就一直为零导致死锁。 ​ 解决这个问题，TCP为每一个连接设置一个持续计时器（persistence timer）。只要TCP的一方收到对方的零窗口通知，就启动该计时器，周期性的发送一个零窗口探测报文段。对方就在确认这个报文的时候给出现在的窗口大小（注意：TCP规定，即使设置为零窗口，也必须接收以下几种报文段：零窗口探测报文段、确认报文段和携带紧急数据的报文段）。 拥塞控制慢开始和拥塞避免 ps：拥塞窗口：cwnd（Congestion Window)是发送端**根据自己估计的网络拥塞程度而设置的窗口值，是来自发送端的流量控制。 如果发现RTT在增大，Vegas就认为网络正在发生拥塞 （1）慢开始原理 ​ 1）当主机开始发送数据时，如果立即将较大的发送窗口的全部数据字节都注入到网络中，那么由于不清楚网络的情况，有可能引其网络拥塞 ​ 2）比较好的方法是试探一下，即从小到达逐渐增大发送端的拥塞控制窗口数值 ​ 3）通常在刚刚开始发送报文段时可先将拥塞窗口cwnd(拥塞窗口)设置为一个最大报文段的MSS的数值。在每收到一个对新报文段确认后，将拥塞窗口增加至多一个MSS的数值，当rwind（接收窗口）足够大的时候，为了防止拥塞窗口cwind的增长引起网络拥塞，还需要另外一个变量—慢开始门限ssthresh （2）拥塞避免原理 ​ 1）让拥塞窗口cwnd缓慢增大，每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。按线性规律缓慢增长。 无论，在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞之后，就会把门限ssthresh设置为发送方发送窗口值的一半 快重传和快恢复 快重传算法：在某些情况下更早的重传丢失的报文段（如果当发送端接收到三个重复的确认ACK时，则断定分组丢失，立即重传丢失的报文段，而不必等待重传计时器超时）。 例如：M1，M2，M3 —–&gt; M1,M3,缺失M2，则接收方向发送方持续发送M2重复确认，当发送方收到M2的三次重复确认，则认为M2报文丢失，启动快重传机制，重传数据，其他数据发送数据放入队列，待快重传结束后再正常传输。 快恢复算法有以下两个要点： ​ 1）当发送方连续收到接收方发来的三个重复确认时，就执行“乘法减小”算法，把慢开始门限减半，这是为了预防网络发生拥塞。 ​ 2）由于发送方现在认为网络很可能没有发生拥塞，因此现在不执行慢开始算法，而是把cwnd(拥塞窗口)值设置为慢开始门限减半后的值，然后开始执行拥塞避免算法，使拥塞窗口的线性增大。 ps: 超时重传是TCP协议保证数据可靠性的另一个重要机制，其原理是在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的ACK报文，那么就重新发送数据，直到发送成功为止。 应用层 协议规定应用进程间标准 DNS 域名解析成IP地址 域名服务器间的查询 迭代查询 递归查询 域名服务器广泛使用高速缓存，减轻根域名服务器以及减少因特网上DNS查询报文数量 DHCP整体流程：主机A以广播形式发送报文，DHCP中继（通常是一个路由器）收到报文之后，以单播形式转发此报文给DHCP服务器，然后又通过中继回发提供报文给主机A。DHCP客户就可以使用被分配的IP地址，但是有一定的租用期 详细的工作步骤： DHCP服务器被动打开UDP端口67，等待客户端发送报文 DHCP客户从UDP端口68发送DHCP发现报文 凡是收到DHCP发现报文的DHCP服务器都发出DHCP提供报文（DHCP客户端可以收到多个DHCP提供报文） DHCP从几个DHCP服务器中选择一个，并向选择的DHCP服务器发送DHCP请求报文 被选择的DHCP服务器发送确认报文。这个时候，DHCP客户就可以使用IP地址了。这是一种已绑定的状态，即DHCP客户的IP地址和硬件地址已经绑定了。这个时候DHCP客户就会根据服务器提供的租用期设置两个计时器，分别在0.5T和0.85T的时候，请求更新租用期。 租用期过了一半，DHCP就会发送请求报文要求更新租用期 DHCP服务器若同意，则发回确认报文，得到新的租用期，重新设置计时器 若不同意，则发回否认报文。这时DHCP客户必须立即停止使用原来的IP地址，然后重新申请新的IP地址。 若服务器不响应，则在0.85T的时候重新执行以上操作 DHCP客户可以随时提前终止服务器所提供的租用期，只需要发送释放报文即可。 HTTP HTTP协议是无状态的，即同一个客户第二次访问同一个服务器上的页面的时候，服务器响应时间与第一次响应时间相同。 HTTP请求报文是作为TCP三次握手的第三个报文的数据 HTTP1.0非持续连接：每次请求完就会关闭TCP连接 缺点： 每请求一个文档就需要两倍RTT的开销。 每一次建立新的TCP连接都要分配缓存和变量 现在浏览器都提供了能够打开5-10个并行的TCP连接，每一个TCP连接处理一个客户请求 HTTP1.1使用持续连接：就是万维网服务器在发送响应后仍然在一段时间内保持这条连接，使同一个客户可以继续在这条连接上传送后续的HTTP报文 两种工作方式： 非流水线方式 特点： 客户收到前一个响应后才能发出下一个请求 缺点： 服务器在发送完一个对象后，TCP连接会空闲，浪费资源 流水线方式 特点： 客户收到HTTP响应报文之前就可以继续发送新的请求报文。 优点： TCP连接的空闲时间减少 两者区别（http://blog.csdn.net/forgotaboutgirl/article/details/6936982） 可扩展性 缓存 带宽优化 长连接 消息传递 Host头域 错误提示 内容协商 HTTP2.0多路复用 HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。 当然HTTP1.1也可以多建立几个TCP连接，来支持处理更多并发的请求，但是创建TCP连接本身也是有开销的。 关于多路复用，可以稍微扯上NIO 数据压缩 HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快。 输入URL之后的网络操作输入URL之后的网络操作：https://segmentfault.com/a/1190000006879700 DNS解析过程中同时存在UDP和TCP请求：http://www.cnblogs.com/549294286/p/5172435.html DNS解析 可扯上为什么DNS解析过程中同时存在UDP和TCP请求 可扯上DNS优化，即缓存 ：浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存 可扯上DNS负载均衡，即DNS重定向。CDN技术就是利用DNS重定向技术的， 建立TCP连接 发送HTTP请求 发送所需文件给客户端 释放连接 浏览器渲染WEB资源","categories":[{"name":"计网","slug":"计网","permalink":"https://kiddot.github.io/categories/计网/"}],"tags":[{"name":"计网","slug":"计网","permalink":"https://kiddot.github.io/tags/计网/"}]},{"title":"HTTP","slug":"计网/HTTP状态码","date":"2017-07-22T04:26:11.000Z","updated":"2018-03-05T16:41:51.698Z","comments":true,"path":"2017/07/22/计网/HTTP状态码/","link":"","permalink":"https://kiddot.github.io/2017/07/22/计网/HTTP状态码/","excerpt":"HTTP","text":"HTTP URI URL URN URL：URL属于URI，不但可以标识Web资源的位置，还指定了具体的操作和获取方式。同时指出了主要访问机制和网络位置 URN：URN也属于URI，用特定命名空间的名字标识资源，使用URN可以在不知道其网络位置及访问方式的情况下讨论资源。 for example 12345678// URI 一个标识http://test.io/posts/hello.html#intro// URL 一个位置标识http://test.io/posts/hello.html// URN 一个名称标识test.io/posts/hello.html#intro 初始版本的HTTP协议每进行一次HTTP通信就需要断开一次TCP连接，但是随着时代变迁，HTTP需要传输的东西多了起来，比如我们现在访问一个页面，需要发送的HTTP请求是几十级别的，因此要进行几十次级别的TCP三次握手和四次挥手，这将造成很多无谓的通信开销，所以HTTP/1.1和部分的HTTP/1.0实现了持久连接，就是HTTP一次传输完毕之后，只要没有说明确要断开，则会继续保持TCP连接。并且长连接让HTTP的管线化成为可能。管线化就是并行请求，不必等一个请求结束再开始另一个。 在HTTP/1.1中，所有连接都默认为持久连接，我们可以看到所有请求的请求头和响应头都带有Connection字段，默认为keep-alive，就是默认保持持久连接。当我们显式将其置为close的时候，将会关闭持久连接的方式。 HTTP协议会通过编码把实体压缩，并在网络上传输，这样可以提高传输速率。提高传输速率的另一种方式就是，在面对比较大的实体时，HTTP会选择采用编码分割发送的方式，将实体分割成一个个块，又服务端接受并且解码，让服务端可以逐步展示页面。 HTTP协议可以进行范围请求，服务端发的HTTP请求的头部带有一个字段Range来规定请求的范围。比如Range: bytes=5001-，服务端返回的响应的HTTP请求使用Content-Range来指定返回的范围，范围请求成功的状态码是206 Partial Content，若范围请求不成功。则会返回200 OK并且整个文件返回。 HTTP协议还会进行内容协商返回。通过Accept，Accept-Charset，Accept-Encoding，Accept-Language，Content-Language这些包含在请求头部的字段，让服务端鉴别返回适合的内容（比如某网站的英文版或者中文版）。内容协商还分为三种类型 服务器驱动，通过以上几个请求头鉴别返回内容 客户端驱动协商，在页面上显示选项让用户选择 透明协商，结合以上两种方法 HTTP状态码状态码通常会包含在响应报文的第一行，状态码是对返回的请求结果的一个简单描述。 粗略总结状态码类型： 1XX：信息性状态码，接受的请求正在处理 2XX：成功状态码，请求正常处理完毕 3XX：重定向状态码，需要进行附加操作以完成请求 4XX：客户端错误状态码，服务器无法处理请求 5XX：服务器错误状态码，服务器处理请求出错 具体到个别常用的状态码。 1XX 100 Continue：客户端应继续发送请求。 101 Switching Protocols：需要切换协议，服务器通过的Upgrade响应头字段通知客户端。 HTML5引入的WebSocket便是这样工作的。首先客户端请求websocket所在的URL，服务器返回101，然后便建立了全双工的TCP连接。 注意Upgrade和Connection头字段属于Hop-by-hop字段，设置Websocket代理时需要继续设置这两个字段，而不是简单地转发请求。 2XX 成功 200 OK：最常见的状态码，表示请求被正常地成功处理了。但是在响应的报文中，会因为请求方法不同而不同，因为我们都知道用GET和HEAD方法获取的东西不同，GET会获取到实体主体。而HEAD方法只会让响应返回头部，并不会返回实体主体。 204 No Content：表示请求已被成功处理，但是返回的响应报文中不包含实体的主体部分。也就是没有内容。一般用在客户端向服务端发送信息，但是服务端不需要发送新信息的情况。 206 Partial Content：表示客户端进行了范围请求，而服务器成功执行了这部分的GET请求。 3XX 重定向 301 Moved Permanently：永久性重定向，表示请求的资源已被分配了新的URI，会自动重定向到新的URI，新的URI将会作为响应头的Location字段返回。并且如果用户此前将这个页面收藏为了书签，则301会帮助用户更新书签。 302 Found：临时性重定向，表示请求的资源已被分配了新的URI，希望本次能使用新的URI访问。与301不同的是，因为302表示的是临时性质，所以并不会去更新用户的书签。 303 See Other：该状态码表示由于请求对应的资源存在另一个URI，应使用GET方法定向获取请求的资源。303与302不同之处在于，302是不会改变请求的方法，如果请求方法是POST的话，重定向的请求也应该是POST。而对于303，使用POST请求的话，重定向的请求应该是GET请求。 304 Not Modified：服务端资源未改变，可直接使用客户端未过期的缓存。304和重定向没关系。 307 Temporary Redirect：临时重定向，和302有相同的含义。不同之处在于，307状态码并不会指定客户端要用什么样的请求方法请求重定向地址。 4XX 客户端错误 400 Bad Request：表示请求报文中存在语法错误，服务器无法理解这一次的请求。（有可能是参数错误之类的）当错误发生时，需修改请求的内容后再次发送请求。 401 Unauthorized：未授权，该状态码表示发送的请求需要通过认证。服务端会返回401并且在首部带有一个WWW-Authenticate的字段用以指明服务器需要哪种方式的认证。当客户端再次请求该资源的时候，需要在请求头中的Authorization包含认证信息。另外，若服务器第二次请求本URI仍然返回401，则表示用户认证失败。 403 Forbidden：该状态码表示了对请求资源的访问被服务器拒绝了。未获得文件系统的访问权限，访问权限出现某些问题，从未授权的发送源IP地址试图访问等情况都可能发生403响应。 404 Not Found：表明服务器上无法找到请求的资源，也可以使用在拒绝请求的时候。 5XX 服务器错误 500 Internal Server Error：该状态码表明服务器端在执行请求时发生错误。也有可能是Web应用存在的bug。 503 Service Unavailable：该状态码表明服务器暂时处于高负载或者正在进行停机维护，暂时无法处理请求。可以通过返回的首部的Retry-After字段告诉客户端什么时候好。 因为部分功能性状态码需要由服务器维护人员显式根据需求设定，没有设定的话则又HTTP默认会变为200 OK或者404 Not Found或者500 Internal Server Error，这都是HTTP通过一些内置的判断生成的默认值。所以有时候，返回的状态码和响应时错误的，比如有时候服务器内部发生错误，但是状态码依旧返回200 OK。因为状态码有时候会通过HTTP默认设置，但是真实情况并不是这样子的，而在服务器那边并没有显示设定状态码导致了这种情况。 HTTP协作的Web服务器HTTP通信过程中除了客户端和服务端还可以存在 代理服务器 网关 隧道 代理服务器主要可以实现资源缓存（主要是响应返回时进行静态资源的缓存，缓存在代理的服务器中，但是会有缓存的时限，通过HTTP的一些头部协作），也可以实现一些类似访问限制之类的需求 网关能够使给服务器提供非HTTP协议服务，还可以提高通信的安全性，因为通过网关可以对通信线路进行加密 隧道可以确保客户端和服务端进行安全的（远距离的）通信 HTTP的头部具体可分为： 通用的头部字段：请求响应都可以使用的头部字段 请求用的头部字段：从客户端发送的请求会用到的头部字段，补充请求的附加内容，客户端信息之类的信息 响应的头部字段：从服务端返回的响应会用到的头部字段，补充响应的附加内容 实体头部字段：针对请求和响应报文的实体部分使用的头部，补充资源内容更新时间和实体有关的信息。 也可根据缓存代理和非缓存代理的行为分为2种类型 端到端头部：这一类头部会转发给最终接受目标，且会保存在由缓存生成的响应中 逐跳头部：这类头部只会对单次转发有效，会因为缓存或代理不再转发。具体头部如下 Connection Keep-Alive Proxy-Authenticate Proxy-Authorization Trailer TE Transfer-Encoding Upgrade 认识HTTPS HTTP有很多不安全的地方，比如：传输过程中可能被认为窃听，通信的双方不能确认彼此真实身份，传输的报文的完整性不能得到保证。 所以 HTTPS = HTTP + 加密（握手建立连接后进行对称密码的加密传输） + 认证（CA证书） + 完整性保护（数字签名） 听起来好像牛逼了许多，但是HTTPS就是HTTP在通信接口部分用SSL和TLS协议代替了。HTTPS的通信变成了，HTTP先和SSL通信，然后SSL再和TCP通信，多一层SSL从而进行对HTTP通信的保护。 共享秘钥：把秘钥通过HTTP传输到对方，再让对方使用秘钥解密报文，但是把秘钥也发送出去这个过程也会有点点危险，可能会被盗取 公开密钥：使用一对秘钥。接收方把自己的公钥发给发送方，发送方用对方的公钥进行加密，发过去后由对方的私钥解密，但是这样做开销较大，速度较慢 HTTPS结合了这两种加密方式，进行混合加密：使用公开密钥的方式把共享的密钥通过安全的方式发给对方，然后通过共享秘钥进行消息的传输。 通常来说，服务端给客户端发送的第一次的公开密钥未必是真正的公开密钥，所以我们需要借助第三方的认证机构，为服务端生成公开密钥证书，然后服务端发送公开密钥证书过来客户端，客户端通过提前植入浏览器的私钥认证证书的有效性（认证），从而确认公开密钥的安全性，然后通过公开密钥加密共享秘钥，传送过去服务端，服务端通过自己的私钥解密，从而使双方都获取安全的共享秘钥（加密），然后就可以通过安全的共享秘钥进行信息传输。同时应用层会附加一种MAC的报文摘要，以确认报文的完整性（完整性保护） 关于数字签名的完整性保护：通过发送方对原文进行hash运算产生一个摘要信息，然后通过发送方的私钥进行加密，从而产生数字签名。接收方需要对接受的原文进行hash运算产生出一个摘要信息，再用发送方的公钥对数字签名进行解密，对比得出信息是否被篡改。 但是：如果只有数字签名，并不一定能保证发送方的可靠性，因为接收方不能确定所使用解密的公钥就是真正的发送方的公钥。所以需要数字证书，数字证书是一个权威的值得信赖的第三方机构(一般是由政府审核并授权的机构)来统一对外发放主机机构的公钥。 有了这个正确的发送方真实性保证，便可以通过数字签名准确地鉴别信息完整性了。 HTTPS的缺点：由于要进行SSL通信，加密解密等操作，HTTPS与纯文本通信相比消耗更多的CPU和内存，简单来说就是开销会变大。因此，如果是非敏感信息则进行HTTP通信，只有在包含个人信息等敏感通信时才进行HTTPS通信 HTTPS的缺点的优化：由于HTTPS的主要性能消耗在于建立一个新的SSL连接的一个建立协商对称秘钥的过程（因为这其中使用了非对称密码体制交换信息，所以开销比较大）。有两种方法可以恢复原来的session，复用原来建立好的连接，不用重新握手生成秘钥 Session ID：每一次对话都有一个编号（Session ID）。如果对话中断，下次重连的时候，只要客户端给出这个编号，且服务器有这个编号的记录，双方就可以重新使用已有的”对话密钥”，而不必重新生成一把。这个方法适用于所有浏览器，但是如果客户端的请求发到另一台服务器，就无法恢复对话（往往在负载均衡的做法下难以发挥优势） Session Ticket：客户端发送一个服务器在上一次对话中发送过来的Session Ticket。这个Session Ticket是加密的，只有服务器才能解密，其中包括本次对话的主要信息，比如对话密钥和加密方法。当服务器收到Session Ticket以后，解密后就不必重新生成对话密钥了。不过目前只有Firefox和Chrome支持这个方法，但是不同服务端也可快速建立握手。值得注意的是，为了让一台服务器生成的 Session Ticket 能被另外服务器承认，往往需要对 Web Server 进行额外配置。","categories":[{"name":"计网","slug":"计网","permalink":"https://kiddot.github.io/categories/计网/"}],"tags":[{"name":"计网","slug":"计网","permalink":"https://kiddot.github.io/tags/计网/"}]},{"title":"学习AbstractQueuedSynchronizer","slug":"并发/学习AbstractQueuedSynchronizer","date":"2017-07-19T14:22:11.000Z","updated":"2018-03-05T16:46:14.876Z","comments":true,"path":"2017/07/19/并发/学习AbstractQueuedSynchronizer/","link":"","permalink":"https://kiddot.github.io/2017/07/19/并发/学习AbstractQueuedSynchronizer/","excerpt":"学习AbstractQueuedSynchronizer","text":"学习AbstractQueuedSynchronizer 笔记来源：http://blog.zhangjikai.com/2017/04/15/%E3%80%90Java-%E5%B9%B6%E5%8F%91%E3%80%91%E8%AF%A6%E8%A7%A3-AbstractQueuedSynchronizer/ 12&gt; AbstractQueuedSynchronizer 是JUC 中通过 Sync Queue(并发安全的 CLH Queue), Condition Queue(普通的 list) , volatile 变量 state 提供的 控制线程获取统一资源(state) 的 Synchronized 工具.&gt; AQS 和 synchronized​ 在介绍 AQS 的使用之前，需要首先说明一点，AQS 同步和 synchronized 关键字同步（以下简称 synchronized 同步）是采用的两种不同的机制。首先看下 synchronized 同步，synchronized 关键字经过编译之后，会在同步块的前后分别形成 monitorenter 和 monitorexit 这两个字节码指令，这两个字节码需要关联到一个监视对象，当线程执行 monitorenter 指令时，需要首先获得获得监视对象的锁，这里监视对象锁就是进入同步块的凭证，只有获得了凭证才可以进入同步块，当线程离开同步块时，会执行 monitorexit 指令，释放对象锁。 ​ 在 AQS 同步中，使用一个 int 类型的变量 state 来表示当前同步块的状态。以独占式同步（一次只能有一个线程进入同步块）为例，state 的有效值有两个 0 和 1，其中 0 表示当前同步块中没有线程，1 表示同步块中已经有线程在执行。当线程要进入同步块时，需要首先判断 state 的值是否为 0，假设为 0，会尝试将 state 修改为 1，只有修改成功了之后，线程才可以进入同步块。 注意上面提到的两个条件： state 为 0，证明当前同步块中没有线程在执行，所以当前线程可以尝试获得进入同步块的凭证，而这里的凭证就是是否成功将 state 修改为 1（在 synchronized 同步中，我们说的凭证是对象锁，但是对象锁的最终实现是否和这种方式类似，没有找到相关的资料） 成功将 state 修改为 1，通过使用 CAS 操作，我们可以确保即便有多个线程同时修改 state，也只有一个线程会修改成功。 ​ 当线程离开同步块时，会修改 state 的值，将其设为 0，并唤醒等待的线程。所以在 AQS 同步中，我们说线程获得了锁，实际上是指线程成功修改了状态变量 state，而线程释放了锁，是指线程将状态变量置为了可修改的状态（在独占式同步中就是置为了 0），让其他线程可以再次尝试修改状态变量。在下面的表述中，我们说线程获得和释放了锁，就是上述含义， 这与 synchronized 同步中说的获得和释放锁的含义不同，需要区别理解。 同步队列 ​ 对于获取锁失败的线程，可以抽象成一个数据结构，然后把它们存储在一个队列（就是一个FIFO的同步队列）（双向链表）当中。这样就可以方便管理同步状态。 ​ AQS 其实就是依赖内部的同步队列（一个 FIFO的双向链表）来完成同步状态的管理，当前线程获取同步状态失败时，AQS会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把队列中第一个等待节点线程唤醒，使其再次尝试获取同步状态。 Node节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354static final class Node &#123; /** * 标志是独占式模式还是共享模式 */ static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; /** * 线程等待状态的有效值 */ static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; /** * 线程状态，合法值为上面 4 个值中的一个 */ volatile int waitStatus; /** * 当前节点的前置节点 */ volatile Node prev; /** * 当前节点的后置节点 */ volatile Node next; /** * 当前节点所关联的线程 */ volatile Thread thread; /** * 指向下一个在某个条件上等待的节点，或者指向 SHARE 节点，表明当前处于共享模式 */ Node nextWaiter; final boolean isShared() &#123; return nextWaiter == SHARED; &#125; final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125; Node() &#123; // Used to establish initial head or SHARED marker &#125; Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125; 在 Node 类中定义了四种等待状态： CANCELED： 1，因为等待超时 （timeout）或者中断（interrupt），节点会被置为取消状态。处于取消状态的节点不会再去竞争锁，也就是说不会再被阻塞。节点会一直保持取消状态，而不会转换为其他状态。处于 CANCELED 的节点会被移出队列，被 GC 回收。 SIGNAL： -1，表明当前的后继结点正在或者将要被阻塞（通过使用 LockSupport.pack 方法），因此当前的节点被释放（release）或者被取消时（cancel）时，要唤醒它的后继结点（通过 LockSupport.unpark 方法）。 CONDITION： -2，表明当前节点在条件队列中，因为等待某个条件而被阻塞。 PROPAGATE： -3，在共享模式下，可以认为资源有多个，因此当前线程被唤醒之后，可能还有剩余的资源可以唤醒其他线程。该状态用来表明后续节点会传播唤醒的操作。需要注意的是只有头节点才可以设置为该状态（This is set (for head node only) in doReleaseShared to ensure propagation continues, even if other operations have since intervened.）。 0：新创建的节点会处于这种状态 ​ 每次出队时,都是从head节点出队, 入队时. 都是从tail节点插入. 头尾节点也是被volatile修饰来保证他们在多线程环境下的可见性. 独占锁的获取和释放独占锁获取 123456789public final void acquire(int arg) &#123; // 首先尝试获取锁，如果获取失败，会先调用 addWaiter 方法创建节点并追加到队列尾部 // 然后调用 acquireQueued 阻塞或者循环尝试获取锁 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg))&#123; // 在 acquireQueued 中，如果线程是因为中断而退出的阻塞状态会返回 true // 这里的 selfInterrupt 主要是为了恢复线程的中断状态 selfInterrupt(); &#125;&#125; acquire 会首先调用 tryAcquire 方法来获得锁，该方法需要我们来实现。如果没有获取锁，会调用 addWaiter 方法创建一个和当前线程关联的节点追加到同步队列的尾部，我们调用 addWaiter 时传入的是 Node.EXCLUSIVE，表明当前是独占模式。 ​ 123456789101112131415private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // tail 指向同步队列的尾节点 Node pred = tail; // Try the fast path of enq; backup to full enq on failure if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; ​ addWaiter 方法会首先调用 if 方法，来判断能否成功将节点添加到队列尾部，如果添加失败，再调用 enq 方法（使用循环不断重试）进行添加。 ​ addWaiter 仅仅是将节点加到了同步队列的末尾，并没有阻塞线程，线程阻塞的操作是在 acquireQueued 方法中完成的。 123456789101112131415161718192021222324252627final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // 如果当前节点的前继节点是 head，就使用自旋（循环）的方式不断请求锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 成功获得锁，将当前节点置为 head 节点，同时删除原 head 节点 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // shouldParkAfterFailedAcquire 检查是否可以挂起线程， // 如果可以挂起进程，会调用 parkAndCheckInterrupt 挂起线程， // 如果 parkAndCheckInterrupt 返回 true，表明当前线程是因为中断而退出挂起状态的， // 所以要将 interrupted 设为 true，表明当前线程被中断过 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; shouldParkAfterFailedAcquire 会检查前继节点的等待状态，如果前继节点状态为 SIGNAL，则可以将当前节点关联的线程挂起，如果不是 SIGNAL，会做一些其他的操作，在当前循环中不会挂起线程。如果确定了可以挂起线程，就调用 parkAndCheckInterrupt 方法对线程进行阻塞： 12345678910private final boolean parkAndCheckInterrupt() &#123; // 挂起当前线程 LockSupport.park(this); // 可以通过调用 interrupt 方法使线程退出 park 状态， // 为了使线程在后面的循环中还可以响应中断，会重置线程的中断状态。 // 这里使用 interrupted 会先返回线程当前的中断状态，然后将中断状态重置为 false， // 线程的中断状态会返回给上层调用函数，在线程获得锁后， // 如果发现线程曾被中断过，会将中断状态重新设为 true return Thread.interrupted();&#125; 独占锁释放 release 方法，可以释放互斥锁： 12345678910public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; // waitStatus 为 0，证明是初始化的空队列或者后继结点已经被唤醒了 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; ​ 在独占模式下释放锁时，是没有其他线程竞争的，所以处理会简单一些。首先尝试释放锁，如果失败就直接返回（失败不是因为多线程竞争，而是线程本身就不拥有锁）。如果成功的话，会检查头结点的状态，然后调用 unparkSuccessor 方法来唤醒后续线程。 unparkSuccessor 的实现： 12345678910111213141516171819202122private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; // 将 head 节点的状态置为 0，表明当前节点的后续节点已经被唤醒了， // 不需要再次唤醒，修改 ws 状态主要作用于 release 的判断 if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; ​ 在 unparkSuccessor 方法中，如果发现头节点的后继结点为 null 或者处于 CANCELED 状态，会从尾部往前找（在节点存在的前提下，这样一定能找到）离头节点最近的需要唤醒的节点，然后唤醒该节点。 共享锁获取和释放共享锁的释放 ​ tryReleaseShared 的方法由我们自己重写，该方法的主要功能就是修改共享资源的数量（state + releases），因为可能会有多个线程同时释放资源，所以实现的时候，一般采用循环加 CAS 操作的方式，如下面的形式： 12345678910protected boolean tryReleaseShared(int releases) &#123; // 释放共享资源，因为可能有多个线程同时执行，所以需要使用 CAS 操作来修改资源总数。 for (;;) &#123; int lastCount = getState(); int newCount = lastCount + releases; if (compareAndSetState(lastCount, newCount)) &#123; return true; &#125; &#125;&#125; 当共享资源数量修改了之后，会调用 doReleaseShared 方法，该方法主要唤醒同步队列中的第一个等待节点（head.next），下面是具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; // head = null 说明没有初始化，head = tail 说明同步队列中没有等待节点 if (h != null &amp;&amp; h != tail) &#123; // 查看当前节点的等待状态 int ws = h.waitStatus; // 我们在前面说过，SIGNAL说明有后续节点需要唤醒 if (ws == Node.SIGNAL) &#123; /* * 将当前节点的值设为 0，表明已经唤醒了后继节点 * 可能会有多个线程同时执行到这一步，所以使用 CAS 保证只有一个线程能修改成功， * 从而执行 unparkSuccessor，其他的线程会执行 continue 操作 */ if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) &#123; /* * ws 等于 0，说明无需唤醒后继结点（后续节点已经被唤醒或者当前节点没有被阻塞的后继结点）， * 也就是这一次的调用其实并没有执行唤醒后继结点的操作。就类似于我只需要一张优惠券， * 但是我的两个朋友，他们分别给我了一张，因此我就剩余了一张。然后我就将这张剩余的优惠券 * 送（传播）给其他人使用，因此这里将节点置为可传播的状态（PROPAGATE） */ continue; // loop on failed CAS &#125; &#125; if (h == head) // loop if head changed break; &#125;&#125; 从上面的实现中，doReleaseShared 的主要作用是用来唤醒阻塞的节点并且一次只唤醒一个，让该节点关联的线程去重新竞争锁，它既不修改同步队列，也不修改共享资源。 当多个线程同时释放资源时，可以确保两件事： 共享资源的数量能正确的累加 至少有一个线程被唤醒，其实只要确保有一个线程被唤醒就可以了，即便唤醒了多个线程，在同一时刻，也只能有一个线程能得到竞争锁的资格(在获取共享锁部分解释)。 所以释放锁做的主要工作还是修改共享资源的数量。 共享锁的获取​ 通过 acquireShared 方法，我们可以申请共享锁，下面是具体的实现： 12345public final void acquireShared(int arg) &#123; // 如果返回结果小于 0，证明没有获取到共享资源 if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 如果没有获取到共享资源，就会执行 doAcquireShared 方法： 123456789101112131415161718192021222324252627private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 从上面的代码中可以看到，只有前置节点为 head 的节点才有可能去竞争锁，这点和独占模式的处理是一样的，所以即便唤醒了多个线程，也只有一个线程能进入竞争锁的逻辑，其余线程会再次进入 park 状态。 ps：虽然这里的处理上和独占模式一样的，但是有一些区别的。独占模式获取锁，只能有一个线程获得，直到这个线程释放锁，其它线程才有机会获得锁。共享模式的话，虽然每次只会唤醒一个线程去竞争锁，但是这个线程竞争获得锁之后，即使没有释放这个锁，下一个节点的线程依然有机会去获得锁的资源，这里只是获取锁的时候处理方式有点不一样。类似于一排人，分发试卷，试卷（锁）会一张一张往后面派发，但每次只能有一个人（线程）去获得。 当线程获取到共享锁之后，会执行 setHeadAndPropagate 方法，在这里就不详细说具体实现了。 ​ 总结来说，当一个节点获取到共享锁之后，它除了将自身设为 head 节点之外，还会判断一下是否满足唤醒后继结点的条件，如果满足，就唤醒后继结点，后继结点获取到锁之后，会重复这个过程，直到判断条件不成立。 中断 在获取锁时还可以设置响应中断，独占锁和共享锁的处理逻辑类似，这里我们以独占锁为例。 12345678910111213141516171819202122232425262728public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg);&#125;private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) &#123; // 这里会抛出异常 throw new InterruptedException(); &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; ​ 从上面的代码中我们可以看出，acquireInterruptibly 和 acquire 的逻辑类似，只是在下面的代码处有所不同：当线程因为中断而退出阻塞状态时，会直接抛出 InterruptedException 异常。 ​ 我们知道，不管是抛出异常还是方法返回，程序都会执行 finally 代码，而 failed 肯定为 true，所以抛出异常之后会执行 cancelAcquire 方法，cancelAcquire 方法主要将节点从同步队列中移除。 12345678910111213141516171819202122232425262728293031323334353637private void cancelAcquire(Node node) &#123; // Ignore if node doesn't exist if (node == null) return; node.thread = null; // 跳过前面的已经取消的节点 Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 保存下 pred 的后继结点，以便 CAS 操作使用 // 因为可能存在已经取消的节点，所以 pred.next 不一等于 node Node predNext = pred.next; // Can use unconditional write instead of CAS here. // After this atomic step, other Nodes can skip past us. // Before, we are free of interference from other threads. // 将节点状态设为 CANCELED node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // If successor needs signal, try to set pred's next-link // so it will get one. Otherwise wake it up to propagate. int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 从上面的代码可以看出，节点的删除分为三种情况： 删除节点为尾节点，直接将该节点的第一个有效前置节点置为尾节点 删除节点的前置节点为头节点，则对该节点执行 unparkSuccessor 操作 删除节点为中间节点，结果如下图所示。下图中（1）表示同步队列的初始状态，假设删除 node2， node1 是正常节点（非 CANCELED），（2）就是删除 node2 后同步队列的状态，此时 node1 节点的后继已经变为 node3，也就是说当 node1 变为 head 之后，会直接唤醒 node3。当另外的一个节点中断之后再次执行 cancelAcquire，在执行下面的代码时，会使同步队列的状态由（2）变为（3），此时 node2 已经没有外界指针了，可以被回收了。如果一直没有另外一个节点中断，也就是同步队列一直处于（2）状态，那么需要等 node3 被回收之后，node2 才可以被回收。 超时 超时是在中断的基础上加了一层时间的判断，这里我们还是以独占锁为例。 总结​ AQS利用模板设计模式来为其子类屏蔽了同步状态的管理, 同步队列的管理, 线程的挂起和唤醒等操作, 使得子类只需要关注本身获取同步状态的逻辑. AQS内部总体实现分为两种模式: 共享式 独占式 共享式和独占式都支持不可中断, 可中断, 可中断并且超时获取同步状态.","categories":[{"name":"并发","slug":"并发","permalink":"https://kiddot.github.io/categories/并发/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://kiddot.github.io/tags/并发/"}]},{"title":"synchronized & ReentrantLock","slug":"并发/synchronized & ReentrantLock","date":"2017-07-18T14:22:11.000Z","updated":"2018-03-05T16:42:53.783Z","comments":true,"path":"2017/07/18/并发/synchronized & ReentrantLock/","link":"","permalink":"https://kiddot.github.io/2017/07/18/并发/synchronized & ReentrantLock/","excerpt":"synchronized &amp; ReentrantLock性能对比LYCOG - Performance of ReentrantLock and Synchronized","text":"synchronized &amp; ReentrantLock性能对比LYCOG - Performance of ReentrantLock and Synchronized 为什么竞争激烈时，使用ReentrantLock会更好？ synchronized缺点：1） 只有一个condition与锁相关联，这个condition是什么？就是synchronized对针对的对象锁。 2） 多线程竞争一个锁时，其余未得到锁的线程只能不停的尝试获得锁，而不能中断。这种情况对于大量的竞争线程会造成性能的下降等后果。 改善： 1） 一个ReentrantLock可以有多个Condition实例 2) ReentrantLock提供了lockInterruptibly()方法可以优先考虑响应中断 ReentrantLock与synchronized对比 由于ReentrantLock在提供了多样的同步功能（除了可响应中断，还能设置时间限制），因此在同步比较激烈的情况下，性能比synchronized大大提高。 不过，在同步竞争不激烈的情况下，synchronized还是非常合适的（因为JVM会进行优化，具体不清楚怎么优化的）。 为什么不放弃synchronized 虽然ReentrantLock是个非常动人的实现，相对 synchronized 来说，它有一些重要的优势 在使用 synchronized 的时候，不用忘记释放锁；在退出synchronized块时，JVM 会做这件事 JVM 用 synchronized 管理锁定请求和释放时，JVM 在生成线程转储时能够包括锁定信息。这些对调试非常有价值，因为它们能标识死锁或者其他异常行为的来源。","categories":[{"name":"并发","slug":"并发","permalink":"https://kiddot.github.io/categories/并发/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://kiddot.github.io/tags/并发/"}]},{"title":"SynchronousQueue实现原理","slug":"并发/SynchronousQueue实现原理","date":"2017-07-18T07:21:22.000Z","updated":"2018-03-05T16:46:30.256Z","comments":true,"path":"2017/07/18/并发/SynchronousQueue实现原理/","link":"","permalink":"https://kiddot.github.io/2017/07/18/并发/SynchronousQueue实现原理/","excerpt":"SynchronousQueue实现原理","text":"SynchronousQueue实现原理 原文地址:http://blog.csdn.net/yanyan19880509/article/details/52562039 前言SynchronousQueue是一个比较特别的队列，由于在线程池方面有所应用，为了更好的理解线程池的实现原理，笔者花了些时间学习了一下该队列源码(JDK1.8)，此队列源码中充斥着大量的CAS语句，理解起来是有些难度的，为了方便日后回顾，本篇文章会以简洁的图形化方式展示该队列底层的实现原理。 SynchronousQueue简单使用经典的生产者-消费者模式，操作流程是这样的： 有多个生产者，可以并发生产产品，把产品置入队列中，如果队列满了，生产者就会阻塞； 有多个消费者，并发从队列中获取产品，如果队列空了，消费者就会阻塞； 如下面的示意图所示： SynchronousQueue 也是一个队列来的，但它的特别之处在于它内部没有容器，一个生产线程，当它生产产品（即put的时候），如果当前没有人想要消费产品(即当前没有线程执行take)，此生产线程必须阻塞，等待一个消费线程调用take操作，take操作将会唤醒该生产线程，同时消费线程会获取生产线程的产品（即数据传递），这样的一个过程称为一次配对过程(当然也可以先take后put,原理是一样的)。 我们用一个简单的代码来验证一下，如下所示： 12345678910111213141516171819202122232425262728293031323334353637package com.concurrent;import java.util.concurrent.SynchronousQueue;public class SynchronousQueueDemo &#123; public static void main(String[] args) throws InterruptedException &#123; final SynchronousQueue&lt;Integer&gt; queue = new SynchronousQueue&lt;Integer&gt;(); Thread putThread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;put thread start&quot;); try &#123; queue.put(1); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(&quot;put thread end&quot;); &#125; &#125;); Thread takeThread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;take thread start&quot;); try &#123; System.out.println(&quot;take from putThread: &quot; + queue.take()); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(&quot;take thread end&quot;); &#125; &#125;); putThread.start(); Thread.sleep(1000); takeThread.start(); &#125;&#125;12345678910111213141516171819202122232425262728293031323334353637 一种输出结果如下: 12345put thread starttake thread starttake from putThread: 1put thread endtake thread end12345 从结果可以看出，put线程执行queue.put(1) 后就被阻塞了，只有take线程进行了消费，put线程才可以返回。可以认为这是一种线程与线程间一对一传递消息的模型。 SynchronousQueue实现原理不像ArrayBlockingQueue、LinkedBlockingDeque之类的阻塞队列依赖AQS实现并发操作，SynchronousQueue直接使用CAS实现线程的安全访问。由于源码中充斥着大量的CAS代码，不易于理解，所以按照笔者的风格，接下来会使用简单的示例来描述背后的实现模型。 队列的实现策略通常分为公平模式和非公平模式，接下来将分别进行说明。 公平模式下的模型：公平模式下，底层实现使用的是TransferQueue这个内部队列，它有一个head和tail指针，用于指向当前正在等待匹配的线程节点。初始化时，TransferQueue的状态如下： 接着我们进行一些操作： 1、线程put1执行 put(1)操作，由于当前没有配对的消费线程，所以put1线程入队列，自旋一小会后睡眠等待，这时队列状态如下： 2、接着，线程put2执行了put(2)操作，跟前面一样，put2线程入队列，自旋一小会后睡眠等待，这时队列状态如下： 3、这时候，来了一个线程take1，执行了 take操作，由于tail指向put2线程，put2线程跟take1线程配对了(一put一take)，这时take1线程不需要入队，但是请注意了，这时候，要唤醒的线程并不是put2，而是put1。为何？ 大家应该知道我们现在讲的是公平策略，所谓公平就是谁先入队了，谁就优先被唤醒，我们的例子明显是put1应该优先被唤醒。至于读者可能会有一个疑问，明明是take1线程跟put2线程匹配上了，结果是put1线程被唤醒消费，怎么确保take1线程一定可以和次首节点(head.next)也是匹配的呢？其实大家可以拿个纸画一画，就会发现真的就是这样的。公平策略总结下来就是：队尾匹配队头出队。执行后put1线程被唤醒，take1线程的 take()方法返回了1(put1线程的数据)，这样就实现了线程间的一对一通信，这时候内部状态如下： 4、最后，再来一个线程take2，执行take操作，这时候只有put2线程在等候，而且两个线程匹配上了，线程put2被唤醒，take2线程take操作返回了2(线程put2的数据)，这时候队列又回到了起点，如下所示： 以上便是公平模式下，SynchronousQueue的实现模型。总结下来就是：队尾匹配队头出队，先进先出，体现公平原则。 非公平模式下的模型：我们还是使用跟公平模式下一样的操作流程，对比两种策略下有何不同。非公平模式底层的实现使用的是TransferStack，一个栈，实现中用head指针指向栈顶，接着我们看看它的实现模型: 1、线程put1执行 put(1)操作，由于当前没有配对的消费线程，所以put1线程入栈，自旋一小会后睡眠等待，这时栈状态如下： 2、接着，线程put2再次执行了put(2)操作，跟前面一样，put2线程入栈，自旋一小会后睡眠等待，这时栈状态如下： 3、这时候，来了一个线程take1，执行了take操作，这时候发现栈顶为put2线程，匹配成功，但是实现会先把take1线程入栈，然后take1线程循环执行匹配put2线程逻辑，一旦发现没有并发冲突，就会把栈顶指针直接指向 put1线程 4、最后，再来一个线程take2，执行take操作，这跟步骤3的逻辑基本是一致的，take2线程入栈，然后在循环中匹配put1线程，最终全部匹配完毕，栈变为空，恢复初始状态，如下图所示： 可以从上面流程看出，虽然put1线程先入栈了，但是却是后匹配，这就是非公平的由来。 总结SynchronousQueue由于其独有的线程一一配对通信机制，在大部分平常开发中，可能都不太会用到，但线程池技术中会有所使用，由于内部没有使用AQS，而是直接使用CAS，所以代码理解起来会比较困难，但这并不妨碍我们理解底层的实现模型，在理解了模型的基础上，有兴趣的话再查阅源码","categories":[{"name":"并发","slug":"并发","permalink":"https://kiddot.github.io/categories/并发/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://kiddot.github.io/tags/并发/"}]},{"title":"SparseArray","slug":"java集合/SparseArray","date":"2017-07-16T15:21:12.000Z","updated":"2017-12-14T14:55:52.592Z","comments":true,"path":"2017/07/16/java集合/SparseArray/","link":"","permalink":"https://kiddot.github.io/2017/07/16/java集合/SparseArray/","excerpt":"SparseArray","text":"SparseArray 概述 SparseArray（稀疏数组） 以int为键，Object为值进行一一映射 实现原理特点： 存放key的数组是有序的（二分查找的前提条件） 如果冲突，新值直接覆盖原值，并且不会返回原值（HashMap会返回原值） 如果当前要插入的 key 的索引上的值为DELETE，直接覆盖 前几步都失败了，检查是否需要gc()并且在该索引上插入数据 二分查找算法是核心算法之一 会造成缺点： 在存在大量元素以及涉及大量增删的时候，由于会引起数组的频繁变化 当容器内在大量元素的时候，使用二分查找会带来很差的性能 针对删除时的缺点进行的优化： 删除时，不直接压缩数组；而是对改为进行标记为DELETED（因此，删除并不会造成数组大小的变化，即mSize没有变化） 二分查找算法特点： 当查找的key存在时候，返回的是key在数组位置的index 当查找的key不存在的时候，返回的index是该key应该存在的位置 ### 插入数据的四种情况一、冲突覆盖 上面这个图，插入一个key=3的元素，因为在mKeys中已经存在了这个值，则直接覆盖。 二、插入索引上为DELETED 注意mKeys中并没有 3 这个值，但是通过二分查找得出来，目前应该插入的索引位置为 2 ，即key=4所在的位置，而当前这个位置上对应的value标记为DELETED了，所以会直接将该位置上的key赋值为 3 ，并且将该位置上的value赋值为put()传入的对象。 三、索引上有值，但是应该触发gc() 数组已经满容量了，而且 3 应该插入的位置已经有 4 了，而 5 所指向的值为DELETED，这种情况下，会先去回收DELETED,重新调整数组结构，图中的例子则会回收 5 ,然后再重新计算 3 应该插入的位置 四、满容且无法gc() 这种情况下，就只能对数组进行扩容，然后插入数据。 扩容策略为：创建一个新数组（容量为原来数组的两倍，即×2），然后将原来数组内容复制到新的数组中 可见，数组扩容，会进行数组复制影响效率，尽量避免数组进行扩容 数组紧凑 上面说到，删除只是将values值进行标记，并不会去删除，那么它什么时候会被真正删除？里面有个gc（）方法，就是起到数组紧凑的作用的，即将将原本标记的DELETED对应的key-value都删掉，那什么时候会调用这个gc方法？ 添加元素的时候可能会触发gc()，这个可能的情况就是，容量不足，需要扩容的时候 执行一系列跟index有关的方法，都会试图去触发gc()，这样可以返回给调用者一个精确的索引值。 与HashMap比较优点： 避免了基本数据类型的装箱操作 不需要存储额外的结构体，单个元素的存储成本更低 数据量少情况下，随机访问效率更高 缺点： 插入操作需要复制数组，增删效率低（二分查找算法时间复杂度为O(logn)，hash查找时间复杂度是O(1)） 数据量大时，复制数组成本也更大，gc成本也更大 数据量大时，查询效率也会降低 总结： 从查找、插入、删除速度看，hashmap更佳（虽然数据量小的情况下，两者性能差不多）（主要是一个基于hash查找，一个基于二分查找，两个算法的时间复杂度决定的） 内存开销 HashMap由于自动装箱以及HashEntry额外占用内存，所以HashMap内存占用较大 SparseArray基于数组，不需要自动装箱，内存占用较小 话外题：HashMap的装箱空间效率1. Long的装箱在对象头中，加入额外的指针8Bype，加入8Bype的MarkWord(hashcode与锁信息)，这里就是16Byte 也就是说，long在装箱后，效率为 8/24 = 1/3 2. Map.Entry的装箱字段空间: hash(4) + padding(4) ＋ next(8) = 16Byte，这里的padding是字节对齐 对象头: 16Byte，指针+MarkWord 也就是说，维护一个Entry需要32Byte的空间 1234567static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;&#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;&#125; ​","categories":[{"name":"java集合","slug":"java集合","permalink":"https://kiddot.github.io/categories/java集合/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"https://kiddot.github.io/tags/java集合/"}]},{"title":"ArrayList和LinkedList","slug":"java集合/ArrayList和LinkedList","date":"2017-07-16T15:21:11.000Z","updated":"2017-12-14T06:28:19.813Z","comments":true,"path":"2017/07/16/java集合/ArrayList和LinkedList/","link":"","permalink":"https://kiddot.github.io/2017/07/16/java集合/ArrayList和LinkedList/","excerpt":"ArrayList和LinkedList","text":"ArrayList和LinkedList 区别 ArrayList是实现了基于动态数组的数据结构，而LinkedList是基于链表的数据结构； 对于随机访问get和set，ArrayList要优于LinkedList，因为LinkedList要移动指针； 对于添加和删除操作add和remove，一般大家都会说LinkedList要比ArrayList快，因为ArrayList要移动数据。但是实际情况并非这样，对于添加或删除，LinkedList和ArrayList并不能明确说明谁快谁慢 get ArrayList想要get(int index)元素时，直接返回index位置上的元素，而LinkedList需要通过for循环进行查找，虽然LinkedList已经在查找方法上做了优化，比如index &lt; size / 2，则从左边开始查找，反之从右边开始查找，但是还是比ArrayList要慢。这点是毋庸置疑的。 insert or remove ArrayList想要在指定位置插入或删除元素时，主要耗时的是System.arraycopy动作，会移动index后面所有的元素；LinkedList主耗时的是要先通过for循环找到index，然后直接插入或删除。这就导致了两者并非一定谁快谁慢。 主要有两个因素决定他们的效率，插入的数据量和插入的位置。 ArrayList扩容问题 ArrayList使用一个内置的数组来存储元素，这个数组的起始容量是10.当数组需要增长时，新的容量按如下公式获得：新容量=(旧容量3)/2+1，也就是说每一次容量大概会增长50%。这就意味着，如果你有一个包含大量元素的ArrayList对象，那么最终将有很大的空间会被浪费掉，这个浪费是由ArrayList的工作方式本身造成的。如果没有足够的空间来存放新的元素，数组将不得不被重新进行分配以便能够增加新的元素。*对数组进行重新分配，将会导致性能急剧下降。 回答梳理相同点都是java集合框架，都实现List接口 不同点 ArrayList是实现了基于动态数组的数据结构，它使用索引在数组中搜索和读取数据是很快的。Array获取数据的时间复杂度是O(1),但是要删除数据却是开销很大的，因为这需要重排数组中的所有数据。 LinkedList是基于链表的数据结构，对于添加和删除操作add和remove，一般大家都会说LinkedList要比ArrayList快，因为ArrayList要移动数据。但是实际情况并非这样，对于添加或删除，LinkedList和ArrayList并不能明确说明谁快谁慢 LinkedList需要更多的内存，因为ArrayList的每个索引的位置是实际的数据，而LinkedList中的每个节点中存储的是实际的数据和前后节点的位置。 效率ArrayList： ​ 1.内部的Object类型的影响 ，对于值类型来说，往ArrayList里面添加和修改元素，都会引起装箱和拆箱的操作，频繁的操作可能会影响一部分效率。 ​ 2.数组扩容 ，扩容操作往往会导致不必要的空间浪费，尽量去评估自己需要的容量 ​ 3.频繁的调用IndexOf、Contains等方法（Sort、BinarySearch等方法经过优化，不在此列）引起的效率损失 使用场景使用LinkList的场景： ​ 1.不会随机访问数据。因为如果你需要LinkedList中的第n个元素的时候，你需要从第一个元素顺序数到第n个数据，然后读取数据。 ​ 2.插入和删除元素的操作比较多，读取数据比较少的情况。因为插入和删除元素不涉及重排数据，所以它要比ArrayList要快。 扩展多线问题 ArrayList中的操作不是线程安全的。所以，建议在单线程中才使用ArrayList，而在多线程中可以选择CopyOnWriteArrayList。","categories":[{"name":"java集合","slug":"java集合","permalink":"https://kiddot.github.io/categories/java集合/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"https://kiddot.github.io/tags/java集合/"}]},{"title":"HashMap工作原理以及实现","slug":"java集合/HashMap工作原理及实现","date":"2017-07-16T14:22:11.000Z","updated":"2017-12-14T06:26:51.739Z","comments":true,"path":"2017/07/16/java集合/HashMap工作原理及实现/","link":"","permalink":"https://kiddot.github.io/2017/07/16/java集合/HashMap工作原理及实现/","excerpt":"知识储备：http://www.jianshu.com/p/bf1d7eee28d0（哈希算法总结） 1. 概述从本文你可以学习到：","text":"知识储备：http://www.jianshu.com/p/bf1d7eee28d0（哈希算法总结） 1. 概述从本文你可以学习到： 什么时候会使用HashMap？他有什么特点？ 你知道HashMap的工作原理吗？ 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 你知道hash的实现吗？为什么要这样实现？ 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 当我们执行下面的操作时： 123456789101112HashMap&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();map.put(&quot;语文&quot;, 1);map.put(&quot;数学&quot;, 2);map.put(&quot;英语&quot;, 3);map.put(&quot;历史&quot;, 4);map.put(&quot;政治&quot;, 5);map.put(&quot;地理&quot;, 6);map.put(&quot;生物&quot;, 7);map.put(&quot;化学&quot;, 8);for(Entry&lt;String, Integer&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 运行结果是 政治: 5生物: 7历史: 4数学: 2化学: 8语文: 1英语: 3地理: 6 发生了什么呢？下面是一个大致的结构，希望我们对HashMap的结构有一个感性的认识： 在官方文档中是这样描述HashMap的： Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time. 几个关键的信息：基于Map接口实现、允许null键/值、非同步、不保证有序(比如插入的顺序)、也不保证序不随时间变化。 2. 两个重要的参数在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor) Initial capacity The capacity is the number of buckets in the hash table, The initial capacity is simply the capacity at the time the hash table is created. Load factor The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. 简单的说，Capacity就是buckets的数目，Load factor就是buckets填满程度的最大比例。如果对迭代性能要求很高的话不要把capacity设置过大，也不要把load factor设置过小。当bucket填充的数目（即hashmap中元素的个数）大于capacity*load factor时就需要调整buckets的数目为当前的2倍。 3. put函数的实现put函数大致的思路为： 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor*current capacity)，就要resize。 具体代码的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 节点存在 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 该链为树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 写入 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 超过load factor*current capacity，resize if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 4. get函数的实现在理解了put之后，get就很简单了。大致思路如下： bucket里的第一个节点，直接命中； 如果有冲突，则通过key.equals(k)去查找对应的entry若为树，则在树中通过key.equals(k)查找，O(logn)；若为链表，则在链表中通过key.equals(k)查找，O(n)。 具体代码的实现如下： 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 未命中 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 5. hash函数的实现在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示： 在对hashCode()计算hash时具体实现是这样的： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。其中代码注释是这样写的： Computes key.hashCode() and spreads (XORs) higher bits of hash to lower. Because the table uses power-of-two masking, sets of hashes that vary only in bits above the current mask will always collide. (Among known examples are sets of Float keys holding consecutive whole numbers in small tables.) So we apply a transform that spreads the impact of higher bits downward. There is a tradeoff between speed, utility, and quality of bit-spreading. Because many common sets of hashes are already reasonably distributed (so don’t benefit from spreading), and because we use trees to handle large sets of collisions in bins, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds. 在设计hash函数时，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的(使用&amp;位操作，而非%求余)： 1(n - 1) &amp; hash 设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。 如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题： Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取HashMap的元素时，基本分两步： 首先根据hashCode()做hash，然后确定bucket的index； 如果bucket的节点的key不是我们需要的，则通过keys.equals()在链中找。 在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行get时，两步的时间复杂度是O(1)+O(n)。因此，当碰撞很厉害的时候n很大，O(n)的速度显然是影响速度的。 因此在Java 8中，利用红黑树替换链表，这样复杂度就变成了O(1)+O(logn)了，这样在n很大的时候，能够比较理想的解决这个问题，在Java 8：HashMap的性能提升一文中有性能测试的结果。 6. RESIZE的实现当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的： Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table. 大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。 怎么理解呢？例如我们从16扩展为32时，具体的变化如下所示： 因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 下面是代码的具体实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 7. 总结我们现在可以回答开始的几个问题，加深对HashMap的理解： 1. 什么时候会使用HashMap？他有什么特点？是基于Map接口的实现，存储键值对时，它可以接收null的键值，是非同步的，HashMap存储着Entry(hash, key, value, next)对象。 2. 你知道HashMap的工作原理吗？通过hash的方法，通过put和get存储和获取对象。存储对象时，我们将K/V传给put方法时，它调用hashCode计算hash从而得到bucket位置，进一步存储，HashMap会根据当前bucket的占用情况自动调整容量(超过Load Facotr则resize为原来的2倍)。获取对象时，我们将K传给get，它调用hashCode计算hash从而得到bucket位置，并进一步调用equals()方法确定键值对。如果发生碰撞的时候，Hashmap通过链表将产生碰撞冲突的元素组织起来，在Java 8中，如果一个bucket中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，从而提高速度。 3. 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？通过对key的hashCode()进行hashing，并计算下标( n-1 &amp; hash)，从而获得buckets的位置。如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点 4. 你知道hash的实现吗？为什么要这样实现？在Java 1.8的实现中，是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。 5. 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？如果超过了负载因子(默认0.75)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。 关于Java集合的小抄中是这样描述的： 以Entry[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可得到数组下标。 插入元素时，如果两条Key落在同一个桶(比如哈希值1和17取模16后都属于第一个哈希桶)，Entry用一个next属性实现多个Entry以单向链表存放，后入桶的Entry将next指向桶当前的Entry。 查找哈希值为17的key时，先定位到第一个哈希桶，然后以链表遍历桶里所有元素，逐个比较其key值。 当Entry数量达到桶数量的75%时(很多文章说使用的桶数量达到了75%，但看代码不是)，会成倍扩容桶数组，并重新分配所有原来的Entry，所以这里也最好有个预估值。 取模用位运算(hash &amp; (arrayLength-1))会比较快，所以数组的大小永远是2的N次方， 你随便给一个初始值比如17会转为32。默认第一次放入元素时的初始值是16。 iterator()时顺着哈希桶数组来遍历，看起来是个乱序。 在JDK8里，新增默认为8的閥值，当一个桶里的Entry超过閥值，就不以单向链表而以红黑树来存放以加快Key的查找速度。 参考资料 HashMap的工作原理Java 8：HashMap的性能提升JEP 180: Handle Frequent HashMap Collisions with Balanced TreesConurrentHashMap和Hashtable的区别HashMap和Hashtable的区别 12345678910111213146.6 在Android中使用SparseArray代替HashMap官方推荐使用SparseArray([spɑ:s][ə&apos;reɪ],稀疏的数组)或者LongSparseArray代替HashMap。官方总结有一下几点好处：SparseArray使用基本类型(Primitive)中的int作为Key，不需要Pair&lt;K,V&gt;或者Entry&lt;K,V&gt;这样的包装类，节约了内存;SpareArray维护的是一个排序好的数组，使用二分查找数据，即O(log(N))，每次插入数据都要进行排序，同样耗时O(N)；而HashMap使用hashCode来加入/查找/删除数据，即O(N/buckets_size)；总的来说，就是SparseArray针对Android嵌入式设备进行了优化，牺牲了微小的时间性能，换取了更大的内存优化;同时它还有别的优化，比如对删除操作做了优化；如果你的数据非常少(实际上也是如此)，那么使用SpareArray也是不错的； （整体上）回答梳理1.基本的工作原理put 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor*current capacity)，就要resize。 get bucket里的第一个节点，直接命中； 如果有冲突，则通过key.equals(k)去查找对应的entry 若为树，则在树中通过key.equals(k)查找，O(logn)； 若为链表，则在链表中通过key.equals(k)查找，O(n)。 2.hash函数的实现高16bit不变，低16bit和高16bit做了一个异或 3.Java 8中，利用红黑树替换链表java8中hashmap的性能提升：http://www.importnew.com/14417.html 4.resize、扩充、容量的变化5.Android中使用SparseArray代替HashMap","categories":[{"name":"java集合","slug":"java集合","permalink":"https://kiddot.github.io/categories/java集合/"}],"tags":[{"name":"java集合","slug":"java集合","permalink":"https://kiddot.github.io/tags/java集合/"}]},{"title":"GC方式","slug":"JVM/GC方式","date":"2017-07-14T14:22:11.000Z","updated":"2017-12-13T14:10:15.138Z","comments":true,"path":"2017/07/14/JVM/GC方式/","link":"","permalink":"https://kiddot.github.io/2017/07/14/JVM/GC方式/","excerpt":"GC方式","text":"GC方式 Minor GC新生代(由 Eden and Survivor 组成)的垃圾收集叫做Minor GC。注意： 当jvm 无法为新建对象分配内存空间的时候Minor GC被触发，例如新生代空间被占满。因此新生代空间占用率越高，Minor GC越频繁。 当空间被占满，它下面的所有对象都会被复制，而且堆顶指针从空闲空间的零位置移动。因此取代传统的标记清除压缩算法，去清理Eden区和Survivor区，因此Eden和Survivor区无内存碎片产生。 在Minor GC期间,实际上Tenured区被忽略，实际上Tenured区引用young区的对象被当作GC roots。在标记期间young区引用的Tenured区对象的对象会被忽略。 反对所有Minor GC都会触发“stop-the-world”这一观点。在大多数应用中，忽略”stop-the-world”停留时长。不可否认的是新生代中的一些对象被错误当成垃圾而不会被移动到Survivor/Old区。 Major GC ​ Major GC清理Tenured区，但是HotSpot VM发展了这么多年，外界对各种名词的解读已经完全混乱了且明确的定义，所以对于Major GC需要问清楚是指full GC还是old GC。 Full GC 收集整个堆，包括young gen、old gen、perm gen 针对HotSpot VM的实现，它里面的GC其实准确分类只有两大种： Partial GC：并不收集整个GC堆的模式 Young GC：只收集young gen的GC Old GC：只收集old gen的GC。只有CMS的concurrent collection是这个模式 Mixed GC：收集整个young gen以及部分old gen的GC。只有G1有这个模式 Full GC：收集整个堆，包括young gen、old gen、perm gen（如果存在的话）等所有部分的模式。 Major GC通常是跟full GC是等价的，收集整个GC堆。但因为HotSpot VM发展了这么多年，外界对各种名词的解读已经完全混乱了，当有人说“major GC”的时候一定要问清楚他想要指的是上面的full GC还是old GC。 young GC：当young gen中的eden区分配满的时候触发。注意young GC中有部分存活对象会晋升到old gen，所以young GC后old gen的占用量通常会有所升高。 full GC：当准备要触发一次young GC时，如果发现统计数据说之前young GC的平均晋升大小比目前old gen剩余的空间大，则不会触发young GC而是转为触发full GC（因为HotSpot VM的GC里，除了CMS的concurrent collection之外，其它能收集old gen的GC都会同时收集整个GC堆，包括young gen，所以不需要事先触发一次单独的young GC）；或者，如果有perm gen的话，要在perm gen分配空间但已经没有足够空间时，也要触发一次full GC；或者System.gc()、heap dump带GC，默认也是触发full GC。 Minor、Full GC触发条件Minor GC触发条件：当Eden区满时，触发Minor GC。 Full GC触发条件： （1）调用System.gc时，系统建议执行Full GC，但是不必然执行 （2）老年代空间不足 （3）方法区空间不足 （4）通过Minor GC后进入老年代的平均大小大于老年代的可用内存 （5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 回收对象 ​ 对于用可达性分析法搜索不到的对象，GC并不一定会回收该对象。要完全回收一个对象，至少需要经过两次标记的过程。 第一次标记：对于一个没有其他引用的对象，筛选该对象是否有必要执行finalize()方法，如果没有执行必要，则意味可直接回收。 第二次标记：如果被筛选判定位有必要执行，则会放入FQueue队列，并自动创建一个低优先级的finalize线程来执行释放操作。如果在一个对象释放前被其他对象引用，则该对象会被移除FQueue队列。 Full GC为什么那么慢 元数据区的回收算法效率低，虚拟机规范Class回收条件比较苛刻 Full GC回收新生代、老年代、元数据区/永久代。从这个角度讲，多回收了方法区，增加了总的回收耗时。 ​ 为什么Yong GC比Old GC慢？ 新生代复制算法比较快。Eden区回收时直接全部清空，存活的对象存放到内存容量比较小的s1，少了解决内存碎片整理 加上直接copy的速度，效率很高。 （牺牲时间换空间）老年代标记清除算法会导致内存碎片化，因此就引入了标记整理算法，执行完毕后，存活的对象会按序放置，移动对象的内存地址（重点），来解决碎片化，但是执行时间较长。 ​","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"双亲委派模型","slug":"JVM/双亲委派模型","date":"2017-07-13T14:22:11.000Z","updated":"2017-12-13T13:57:21.076Z","comments":true,"path":"2017/07/13/JVM/双亲委派模型/","link":"","permalink":"https://kiddot.github.io/2017/07/13/JVM/双亲委派模型/","excerpt":"双亲委派模型","text":"双亲委派模型 JVM预定义的三种类型类加载器： 启动（Bootstrap）类加载器：是用本地代码实现的类装入器，它负责将 &lt;Java_Runtime_Home&gt;/lib下面的类库加载到内存中（比如rt.jar）。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作。 标准扩展（Extension）类加载器：是由 Sun 的 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将&lt; Java_Runtime_Home &gt;/lib/ext或者由系统变量 java.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器。 系统（System）类加载器：是由 Sun 的 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。它负责将系统类路径（CLASSPATH）中指定的类库加载到内存中。开发者可以直接使用系统类加载器。 双亲委派机制描述 : 某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。 几点思考 Java虚拟机的第一个类加载器是Bootstrap，这个加载器很特殊，它不是Java类，因此它不需要被别人加载，它嵌套在Java虚拟机内核里面，也就是JVM启动的时候Bootstrap就已经启动，它是用C++写的二进制代码（不是字节码），它可以去加载别的类。 ​ 这也是我们在测试时为什么发现System.class.getClassLoader()结果为null的原因，这并不表示System这个类没有类加载器，而是它的加载器比较特殊，是BootstrapClassLoader，由于它不是Java类，因此获得它的引用肯定返回null。 当Java虚拟机要加载一个类时，到底派出哪个类加载器去加载呢？ 首先当前线程的类加载器去加载线程中的第一个类（假设为类A）。注：当前线程的类加载器可以通过Thread类的getContextClassLoader()获得，也可以通过setContextClassLoader()自己设置类加载器。 如果类A中引用了类B，Java虚拟机将使用加载类A的类加载器去加载类B。 还可以直接调用ClassLoader.loadClass()方法来指定某个类加载器去加载某个类。 委托机制的意义 — 防止内存中出现多份同样的字节码 比如两个类A和类B都要加载System类： 如果不用委托而是自己加载自己的，那么类A就会加载一份System字节码，然后类B又会加载一份System字节码，这样内存中就出现了两份System字节码。 如果使用委托机制，会递归的向父类查找，也就是首选用Bootstrap尝试加载，如果找不到再向下。这里的System就能在Bootstrap中找到然后加载，如果此时类B也要加载System，也从Bootstrap开始，此时Bootstrap发现已经加载过了System那么直接返回内存中的System即可而不需要重新加载，这样内存中就只有一份System的字节码了。 是否能自己写一个System类？ 不可以，因为类加载采用委托机制，这样可以保证爸爸们优先，爸爸们能找到的类，儿子就没有机会加载。而System类是Bootstrap加载器加载的，就算自己重写，也总是使用Java系统提供的System，自己写的System类根本没有机会得到加载。 但是，我们可以自己定义一个类加载器来达到这个目的，为了避免双亲委托机制，这个类加载器也必须是特殊的。由于系统自带的三个类加载器都加载特定目录下的类，如果我们自己的类加载器放在一个特殊的目录，那么系统的加载器就无法加载，也就是最终还是由我们自己的加载器加载。","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"类的加载","slug":"JVM/类的加载","date":"2017-07-13T10:51:22.000Z","updated":"2017-12-13T13:56:11.504Z","comments":true,"path":"2017/07/13/JVM/类的加载/","link":"","permalink":"https://kiddot.github.io/2017/07/13/JVM/类的加载/","excerpt":"类的加载","text":"类的加载 类加载到虚拟机内存到卸载的整个生命周期 整个生命周期包括:加载(Loading)、验证(Verification)、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)7个阶段。其中验证、准备、解析3个部分统称为连接(Linking) 类加载的过程加载阶段加载有两种情况： ①当遇到new关键字，或者static关键字的时候就会发生（他们对应着对应的指令）如果在常量池中找不到对应符号引用时，就会发生加载 ②动态加载，当用反射方法（如class.forName(“类名”)），如果发现没有初始化，则要进行初始化。（注：加载的时候发现父类没有被加载，则要先加载父类） 在加载阶段,虚拟机需要完成以下3件事情: 1)通过一个类的全限定名来获取定义此类的二进制字节流。 2)将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 3)在内存中生成一个代表这个类的java.lang.Class对象,作为方法区这个类的各种数据的访问入口。 连接阶段 加载阶段与连接阶段的部分内容(如一部分字节码文件格式验证动作)是交叉进行的,加载阶段尚未完成,连接阶段可能已经开始,但这些夹在加载阶段之中进行的动作,仍然属于连接阶段的内容,这两个阶段的开始时间仍然保持着固定的先后顺序。 验证 验证是连接阶段的第一步,这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求,并且不会危害虚拟机自身的安全。 从整体上看,验证阶段大致上会完成下面4个阶段的检验动作:文件格式验证、元数据验证、字节码验证、符号引用验证。 文件格式验证 是否以魔数0xCAFEBABE开头。 主、次版本号是否在当前虚拟机处理范围之内。 常量池的常量中是否有不被支持的常量类型(检查常量tag标志)。 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量。 CONSTANT_Utf8_info型的常量中是否有不符合UTF8编码的数据。 Class文件中各个部分及文件本身是否有被删除的或附加的其他信息。 元数据验证 这个类是否有父类(除了java.lang.Object之外,所有的类都应当有父类)。 这个类的父类是否继承了不允许被继承的类(被final修饰的类)。 如果这个类不是抽象类,是否实现了其父类或接口之中要求实现的所有方法。 类中的字段、方法是否与父类产生矛盾(例如覆盖了父类的final字段,或者出现不符合规则的方法重载,例如方法参数都一致,但返回值类型却不同等)。 字节码验证 … 符号引用验证 … 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段,这些变量所使用的内存都将在方法区中进行分配。 这时候进行内存分配的仅包括类变量(被static修饰的变量),而不包括实例变量,实例变量将会在对象实例化时随着对象一起分配在Java堆中。其次,这里所说的初始值“通常情况”下是数据类型的零值 比如 1public static int value=123; ​ 变量value在准备阶段过后的初始值为0而不是123,因为这时候尚未开始执行任何Java方法,而把value赋值为123的putstatic指令是程序被编译后,存放于类构造器()方法之中,所以把value赋值为123的动作将在初始化阶段才会执行。 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程 符号引用(Symbolic References):其实就是class文件常量池中的各种引用，他们按照一定规律指向了对应的类名，或者字段，但是并没有在内存中分配空间 直接引用(Direct References):直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。 初始化​ 简单讲就是执行对象的构造函数，给类的静态字段按照程序的意愿进行初始化，注意初始化的顺序。 ​ 此处的初始化由两个函数完成，一个是,初始化所有的类变量（静态变量），该函数不会初始化父类变量，还有一个是实例初始化函数,对类中实例对象进行初始化","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"GC的三种收集方法","slug":"JVM/GC的三种收集方法","date":"2017-07-12T07:51:22.000Z","updated":"2017-12-13T13:54:37.233Z","comments":true,"path":"2017/07/12/JVM/GC的三种收集方法/","link":"","permalink":"https://kiddot.github.io/2017/07/12/JVM/GC的三种收集方法/","excerpt":"GC的三种收集方法","text":"GC的三种收集方法 标记清除分两个阶段 标记：首先标记出所有需要回收的对象 清除：在标记完成后统一回收所有被标记的对象。 特点： 回收特别快 两个不足： 效率问题，标记与清除两个过程效率不高 空间问题，标记清除之后会产生大量的不连续内存碎片 适合使用： 老年代的回收 标记整理分三个阶段： 标记：首先标记出所有需要回收的对象 整理：让所有存活的对象都向一边移动 清除：直接就清除端边界以外的内存 特点： 回收以后的空间连续 缺点： 整理要花一定时间 适合使用： 老年代的回收 复制算法思路： 将内存分成原始的相等的两份，每次只使用一部分 用完一部分，就将还存活的对象复制到另外一块内存 最后将要回收的内存全部清除 特点： 实现简单 运行高效 不足： 将原来的内存缩小了一半 在存活对象比较多时，要进行较多的复制操作，效率会变低 适合使用： 新生代的回收 优化收集方法对复制算法的优化​ 不是将两块内存分配同等大小，可以将存活率低的区域大一些，而让回收后存活的对象所占的区域小一些，不够的内存由老年代的内存来保证，这样复制算法的空闲的空间减少了。","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"Java对象存活分析","slug":"JVM/Java对象存活分析","date":"2017-07-11T03:51:22.000Z","updated":"2017-12-13T13:53:34.178Z","comments":true,"path":"2017/07/11/JVM/Java对象存活分析/","link":"","permalink":"https://kiddot.github.io/2017/07/11/JVM/Java对象存活分析/","excerpt":"Java对象存活分析 ​ 判断对象是否存活有两种比较常见的方法：引用计数法与可达性分析算法。","text":"Java对象存活分析 ​ 判断对象是否存活有两种比较常见的方法：引用计数法与可达性分析算法。 引用计数法 ​ 引用计数法的逻辑非常简单，但是存在问题，java并不采用这种方式进行对象存活判断。 ​ 引用计数法的逻辑是：在堆中存储对象时，在对象头处维护一个counter计数器，如果一个对象增加了一个引用与之相连，则将counter++。如果一个引用关系失效则counter–。如果一个对象的counter变为0，则说明该对象已经被废弃，不处于存活状态。 这种方法来标记对象的状态会存在很多问题： jdk从1.2开始增加了多种引用方式：软引用、弱引用、虚引用，且在不同引用情况下程序应进行不同的操作。如果我们只采用一个引用计数法来计数无法准确的区分这么多种引用的情况。 除了引用计数法无法解决多种类型引用的问题，循环持有问题才是致命的问题。比如: 如果一个对象A持有对象B，而对象B也持有一个对象A，那发生了类似操作系统中死锁的循环持有，这种情况下A与B的counter恒大于1，会使得GC永远无法回收这两个对象。 可达性分析算法​ 这个算法的基本思路就是通过一系列名为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。 如下情况的对象可以作为GC Roots： 虚拟机栈(栈桢中的本地变量表)中的引用的对象 方法区中的类静态属性引用的对象 方法区中的常量引用的对象 本地方法栈中JNI（Native方法）的引用的对象 HotSpot虚拟机如何实现可达性算法？使用OopMap记录并枚举根节点 HotSpot首先需要枚举所有的GC Roots根节点，虚拟机栈的空间不大，遍历一次的时间或许可以接受，但是方法区的空间很可能就有数百兆，遍历一次需要很久。更加关键的是，当我们遍历所有GC Roots根节点时，我们需要暂停所有用户线程，因为我们需要一个此时此刻的”虚拟机快照”，如果我们不暂停用户线程，那么虚拟机仍处于运行状态，我们无法确保能够正确遍历所有的根节点。所以此时的时间开销过大更是我们不能接受的。 基于这种情况，HotSpot实现了一种叫做OopMap的数据结构，这种数据结构在类加载完成时把对象内的偏移量是什么类型计算出，并且存放下位置，当需要遍历根结点时访问所有OopMap即可。 用安全点Safepoint约束根节点 如果将每个符合GC Roots条件的对象都存放进入OopMap中，那么OopMap也会变得很大，而且其中很多对象很可能会发生一些变化，这些变化使得维护这个映射表很困难。实际上，HotSpot并没有为每一个对象都创建OopMap，只在特定的位置上创建了这些信息，这些位置称为安全点（Safepoints）。 为了保证虚拟机中安全点的个数不算太多也不是太少，主要决定安全点是否被建立的因素是时间。当进行了耗时的操作时，比如方法调用、循环跳转等时会产生安全点。","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"对象的创建、布局与访问","slug":"JVM/对象的创建、布局与访问","date":"2017-06-30T10:51:22.000Z","updated":"2017-12-13T13:57:58.632Z","comments":true,"path":"2017/06/30/JVM/对象的创建、布局与访问/","link":"","permalink":"https://kiddot.github.io/2017/06/30/JVM/对象的创建、布局与访问/","excerpt":"对象的创建、布局与访问","text":"对象的创建、布局与访问 一、创建1new People(); 这行代码背后的JVM操作： 类加载检查​ 检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类的加载过程 为对象分配内存 ​ 对象所需内存的大小在类加载完成后便完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来 ​ 根据Java堆中是否有规整有两种内存的分配方式： 指针分配 Java堆中的内存是规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，分配内存也就是把指针向空闲空间那边移动一段与内存大小相等的距离。 空闲列表 Java堆中的内存不是规整的，已使用的内存和空闲的内存相互交错。虚拟机必须维护一张列表，记录哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。 分配内存时解决并发问题的两种方案 ​ 对象创建在虚拟机中时非常频繁的行为，即使是仅仅修改一个指针指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况 对分配内存空间的动作进行同步处理 把内存分配的动作按照线程划分为在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲(TLAB) 内存空间初始化 虚拟机将分配到的内存空间都初始化为零值（不包括对象头）,如果使用了TLAB，这一工作过程也可以提前至TLAB分配时进行。 内存空间初始化保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 对象设置​ 虚拟机对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头之中。 在上面的工作都完成之后，从虚拟机的角度看，一个新的对象已经产生了。 但是从Java程序的角度看，对象的创建才刚刚开始方法还没有执行，所有的字段都还是零。 一般来说，执行new指令之后会接着执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算产生出来。 二、对象内存的布局 ​ 在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头(Header)、实例数据(Instance Data)和对齐填充(Padding)。 对象头 HotSpot虚拟机的对象头包括两部分信息 第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等 另外一个部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据 ​ 实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。 对齐填充 ​ 对齐填充并不是必然存在的，也没有特定的含义，仅仅起着占位符的作用。 三、对象的访问定位 ​ 建立对象是为了使用对象，我们的Java程序需要通过栈上的引用数据来操作堆上的具体对象。 目前主流的访问方式有使用句柄和直接指针两种。 使用句柄​ 如果使用句柄的话，那么Java堆中将会划分出一块内存来作为句柄池，引用中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。 直接指针​ 如果使用直接指针访问，那么Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而引用中存储的直接就是对象地址。 优势：速度更快，节省了一次指针定位的时间开销。（由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是非常可观的执行成本）","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"Java堆空间的分区","slug":"JVM/Java堆空间的分区","date":"2017-06-28T12:51:11.000Z","updated":"2017-12-13T13:57:38.968Z","comments":true,"path":"2017/06/28/JVM/Java堆空间的分区/","link":"","permalink":"https://kiddot.github.io/2017/06/28/JVM/Java堆空间的分区/","excerpt":"Java堆空间的分区","text":"Java堆空间的分区 Eden区 Eden区位于Java堆的年轻代，是新对象分配内存的地方，由于堆是所有线程共享的，因此在堆上分配内存需要加锁。而Sun JDK为提升效率，会为每个新建的线程在Eden上分配一块独立的空间由该线程独享，这块空间称为TLAB（Thread Local Allocation Buffer）。在TLAB上分配内存不需要加锁，因此JVM在给线程中的对象分配内存时会尽量在TLAB上分配。如果对象过大或TLAB用完，则仍然在堆上进行分配。如果Eden区内存也用完了，则会进行一次Minor GC（young GC）。 Survival from to Survival区与Eden区相同都在Java堆的年轻代。Survival区有两块，一块称为from区，另一块为to区，这两个区是相对的，在发生一次Minor GC后，from区就会和to区互换。在发生Minor GC时，Eden区和Survivalfrom区会把一些仍然存活的对象复制进Survival to区，并清除内存。Survival to区会把一些存活得足够旧的对象移至年老代。 老年代 老年代里存放的都是存活时间较久的，大小较大的对象，因此年老代使用标记整理算法。当年老代容量满的时候，会触发一次Major GC（full GC），回收年老代和年轻代中不再被使用的对象资源。 扩展一一、逃逸分析和TLAB 扩展：new出来的对象并不都是被分配在堆上。但是Java中的逃逸分析和TLAB，可以认为一定会分配在堆上。 TLAB： JVM在内存新生代Eden Space中开辟了一小块线程私有的区域，称作TLAB（Thread-local allocation buffer）。默认设定为占用Eden Space的1%。在Java程序中很多对象都是小对象且用过即丢，它们不存在线程共享也适合被快速GC，所以对于小对象通常JVM会优先分配在TLAB上，并且TLAB上的分配由于是线程私有所以没有锁开销。 二、Java对象分配的过程 编译器通过逃逸分析，确定对象是在栈上分配还是在堆上分配。如果是在堆上分配，则进入选项2. 如果tlab_top + size &lt;= tlab_end，则在在TLAB上直接分配对象并增加tlab_top 的值，如果现有的TLAB不足以存放当前对象则3. 重新申请一个TLAB，并再次尝试存放当前对象。如果放不下，则4. 在Eden区加锁（这个区是多线程共享的），如果eden_top + size &lt;= eden_end则将对象存放在Eden区，增加eden_top 的值，如果Eden区不足以存放，则5. 执行一次Young GC（minor collection）。 经过Young GC之后，如果Eden区任然不足以存放当前对象，则直接分配到老年代。 扩展二JVM 新生代为何需要两个 Survivor 空间？为什么不是0个 Survivor 空间？ 如果没有 Survivor 空间的话，垃圾收集将会怎样进行：一遍新生代 gc 过后，不管三七二十一，活着的对象全部进入老年代，即便它在接下来的几次 gc 过程中极有可能被回收掉。这样的话老年代很快被填满， Full GC 的频率大大增加。我们知道，老年代一般都会被规划成比新生代大很多，对它进行垃圾收集会消耗比较长的时间；如果收集的频率又很快的话，那就更糟糕了。基于这种考虑，虚拟机引进了“幸存区”的概念：如果对象在某次新生代 gc 之后任然存活，让它暂时进入幸存区；以后每熬过一次 gc ，让对象的年龄＋1，直到其年龄达到某个设定的值（比如15岁）， JVM 认为它很有可能是个“老不死的”对象，再呆在幸存区没有必要（而且老是在两个幸存区之间反复地复制也需要消耗资源），才会把它转移到老年代。 设置 Survivor 空间的目的是让那些中等寿命的对象尽量在 Minor GC 时被干掉，最终在总体上减少虚拟机的垃圾收集过程对用户程序的影响。 为什么不是1个 Survivor 空间？ 新生代一般都采用复制算法进行垃圾收集。原始的复制算法是把一块内存一分为二， gc 时把存活的对象从一块空间（From space）复制到另外一块空间（To space），再把原先的那块内存（From space）清理干净，最后调换 From space 和 To space 的逻辑角色 在 HotSpot 虚拟机里， Eden 空间和 Survivor 空间默认的比例是 8:1 。我们来看看在只有一个 Survivor 空间的情况下，这个 8:1 会有什么问题。此处为了方便说明，我们假设新生代一共为 9 MB 。对象优先在 Eden 区分配，当 Eden 空间满 8 MB 时，触发第一次 Minor GC 。比如说有 0.5 MB 的对象存活，那这 0.5 MB 的对象将由 Eden 区向 Survivor 区复制。这次 Minor GC 过后， Eden 区被清理干净， Survivor 区被占用了 0.5 MB ，还剩 0.5 MB 。到这里一切都很美好，但问题马上就来了：从现在开始所有对象将会在这剩下的 0.5 MB 的空间上被分配，很快就会发现空间不足，于是只好触发下一次 Minor GC 。可以看出在这种情况下，当 Survivor 空间作为对象“出生地”的时候，很容易触发 Minor GC ，这种 8:1 的不对称分配不但没能在总体上降低 Minor GC 的频率，还会把 gc 的时间间隔搞得很不平均。 为什么2个 Survivor 空间可以达到要求？ 我们把 Eden : From Survivor : To Survivor 空间大小设成 8 : 1 : 1 ，对象总是在 Eden 区出生， From Survivor 保存当前的幸存对象， To Survivor 为空。一次 gc 发生后： 1）Eden 区活着的对象 ＋ From Survivor 存储的对象被复制到 To Survivor ； 2) 清空 Eden 和 From Survivor ； 3) 颠倒 From Survivor 和 To Survivor 的逻辑关系： From 变 To ， To 变 From 。","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]},{"title":"JVM内存模型以及分区","slug":"JVM/JVM内存模型以及分区","date":"2017-06-28T10:51:22.000Z","updated":"2017-12-13T13:57:49.856Z","comments":true,"path":"2017/06/28/JVM/JVM内存模型以及分区/","link":"","permalink":"https://kiddot.github.io/2017/06/28/JVM/JVM内存模型以及分区/","excerpt":"JVM内存模型以及分区","text":"JVM内存模型以及分区 ​ 学习过C语言都知道，C编译器在划分内存区域的时候经常将管理的区域划分为数据段和代码段，数据段包括堆、栈以及静态数据区。那在Java语言当中，内存又是如何划分的？ ​ 我们在谈Java内存区域划分的时候事实上是指JVM内存区域划分。在讨论JVM内存区域划分之前，先来看一下Java程序具体执行的过程： Java源代码文件(.java后缀)会被Java编译器编译为字节码文件(.class后缀) JVM中的类加载器加载各个类的字节码文件 加载完毕之后，交由JVM执行引擎执行 ​ 在整个程序执行过程中，JVM会用一段空间来存储程序执行期间需要用到的数据和相关信息，这段空间一般被称作为Runtime Data Area（运行时数据区），也就是我们常说的JVM内存 运行时数据区包括哪几部分，以及存储了哪些数据程序计数器 ​ 在汇编语言中，程序计数器是指CPU中的寄存器，它保存的是程序当前执行的指令的地址（也可以说保存下一条指令的所在存储单元的地址），当CPU需要执行指令时，需要从程序计数器中得到当前需要执行的指令所在存储单元的地址，然后根据得到的地址获取到指令，在得到指令之后，程序计数器便自动加1或者根据转移指针得到下一条指令的地址，如此循环，直至执行完所有的指令。 ​ JVM中的程序计数器并不像汇编语言中的程序计数器一样是物理概念上的CPU寄存器，但是JVM中的程序计数器的功能跟汇编语言中的程序计数器的功能在逻辑上是等同的，也就是说是用来指示 执行哪条指令的。 ​ 由于在JVM中，多线程是通过线程轮流切换来获得CPU执行时间的，因此，在任一具体时刻，一个CPU的内核只会执行一条线程中的指令，因此，为了能够使得每个线程都在线程切换后能够恢复在切换之前的程序执行位置，每个线程都需要有自己独立的程序计数器，并且不能互相被干扰，否则就会影响到程序的正常执行次序。因此，可以这么说，程序计数器是每个线程所私有的。 ​ 在JVM规范中规定，如果线程执行的是非native方法，则程序计数器中保存的是当前需要执行的指令的地址；如果线程执行的是native方法，则程序计数器中的值是undefined。 ​ 由于程序计数器中存储的数据所占空间的大小不会随程序的执行而发生改变，因此，对于程序计数器是不会发生内存溢出现象(OutOfMemory) Java栈 与C语言中的数据段中的栈类似 ​ Java栈是Java方法执行的内存模型 ​ Java栈中存放的是一个个的栈帧，每个栈帧对应一个被调用的方法，在栈帧中包括局部变量表(Local Variables)、操作数栈(Operand Stack)、指向当前方法所属的类的运行时常量池（运行时常量池的概念在方法区部分会谈到）的引用(Reference to runtime constant pool)、方法返回地址(Return Address)和一些额外的附加信息。当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈。 ​ 由此可以得出三个结论：一、线程当前执行的方法对应的栈帧必定位于Java栈的顶部。 二、因为栈的空间是有限的，所以在使用递归的时候，会因为执行方法的次数太多导致栈帧不足以存放在栈中而发生栈内存溢出现象，即栈溢出。三、栈空间的分配与释放都是由系统自动实施管理的。 局部变量表 用来存储方法中的局部变量。对于基本数据类型的变量，则直接存储它的值，对于引用类型的变量，则存的是指向对象的引用。局部变量表的大小在编译器就可以确定其大小了，因此在程序执行期间局部变量表的大小是不会改变的 操作数栈 协助完成程序中所有的计算过程 指向运行时常量池的引用 在方法执行的过程中有可能需要用到类中的常量，所以必须要有一个引用指向运行时常量 方法返回地址 当一个方法执行完毕之后，要返回之前调用它的地方，因此在栈帧中必须保存一个方法返回地址 ps：每个线程都会有一个自己的Java栈 本地方法栈 ​ 本地方法栈与Java栈的作用和原理非常相似。区别是Java栈是为执行Java方法服务的，而本地方法栈则是为执行本地方法（Native Method）服务的。 ​ 在JVM规范中，并没有对本地方发展的具体实现方法以及数据结构作强制规定，虚拟机可以自由实现它。在HotSopt虚拟机中直接就把本地方法栈和Java栈合二为一。 堆 ​ 在C语言中，堆这部分空间是唯一一个程序员可以管理的内存区域。但是在java中基本不用关心空间的释放问题，因为Java的GC机制。这部分空间也是Java垃圾收集器管理的主要区域。 ​ Java中的堆是用来存储对象本身的以及数组（数组引用是存放在Java栈中的）。 ​ 堆是被所有线程共享的，在JVM中只有一个堆。 方法区 与堆一样，是被线程共享的区域 ​ 在方法区中，存储了每个类的信息（包括类的名称、方法信息、字段信息）、静态变量、常量以及编译器编译后的代码等。 ​ 在Class文件中除了类的字段、方法、接口等描述信息外，还有一项信息是常量池，用来存储编译期间生成的字面量和符号引用。 ​ 运行时常量池,是方法区中非常重要的部分，它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载的JVM后，对应的运行时常量池就被创建出来。但不仅是Class文件常量池中的内容才能进入运行时常量池，在运行期间也可将新的常量放入运行时常量池中，比如String的intern方法。 在JVM规范中，没有强制要求方法区必须实现垃圾回收。所以许多人称之为永久代。 扩展：JDK7数据存储在永久代的部分数据就已经转移到了Java Heap或者是 Native Heap。JDK8永久代的废弃。 为什么要废除永久代？ 永久代内存经常不够用或发生内存泄露，爆出异常java.lang.OutOfMemoryError: PermGen。由于方法区主要存储类的相关信息，所以对于动态生成类的情况比较容易出现永久代的内存溢出。 字符串存在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。 ​ 用什么代替永久代 元空间。元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。 JDK8中类加载（方法区的功能）已经不在永久代PerGem中了，而是Metaspace（元空间）中","categories":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://kiddot.github.io/tags/jvm/"}]}]}